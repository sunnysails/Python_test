软件学报
JOURNAL OF SOFTWARE
1999年 第1期 No.1 1999



基于复杂特征的VN结构模板获取模型*
赵　军　黄昌宁
　　摘要　提出了基于复杂特征的VN结构模板获取模型.首先用统计决策树模型生长动词分类树,然后用最小描述长度原则对动词分类树剪枝,最后由动词分类树推导出VN结构模板.实验证明,在利用结构模板进行VN结构的识别时,这种模型比基于义类和极大似然估计原则的模型具有更高的精确率和召回率.
　　关键词　自然语言处理,语料库,复杂特征集,统计决策树,最小描述长度原则.
　　中图法分类号　TP18
The Complex-feature-based Model for Acquisition of VN-construction Structure Templates
ZHAO Jun　HUANG Chang-ning
　　Abstract　In this paper, a complex-feature- and MDL-based model for acquisition of VN-construction structure templates is put forward. First, a verb classification tree is created using statistical decision tree model. Then, the tree is pruned based on MDL (minimum description length) principle. Finally, structure templates are derived based on the verb classification tree. The experiments show that using the structure templates acquired with the model to recognizing VN-structure, the system has its advantages over the model based on the sense and the MLE (maximum likelihood estimation) principle in precision and recall.
　　Key words　Natural language processing, corpus, complex feature, statistical decision tree, minimum description length principle.
　　在汉语中,动词V和名词N的同现情况有以下3种：偏正结构VN（如“射门方法”）、动宾结构VO（如“改进方法”）和非法组合IC（如“包括［方法的改进］”中的“包括方法”）.本文把VO结构和非法组合IC统称为～VN结构.正确地识别VN结构对于句法分析、信息检索、信息抽取等都是至关重要的,其中一种重要的方法是利用词语结构模板来识别VN结构,例如,对于“射门方法”、“改进方法”和“包括方法”,如果能够利用某种方法从训练语料中获得如下形式的结构模板：Hh05+“方法”→VN,Ih11+“方法”→VO,Jd05+“方法”→IC（其中Hh05、Ih11和Jd05是《同义词词林》［1］（以下简称《词林》）的义类代码）,则可以正确地识别它们的结构.
　　基于词的VN结构模板获取可以形式化地表示如下：设有动词集合V=｛υ1,υ2,...,υV｝,名词集合N=｛n1,n2,...,nN｝,给定观察数据Ｏ=｛(υ,n)|υ∈V,n∈N｝,求解概率模型p(υ,n),使它能够解释观察数据O.因为这种概率模型的参数数目众多（|N|*|V|）,所以,在参数估计时存在数据稀疏问题.
　　建立基于等价类的概率模型是解决数据稀疏问题的重要方法.这种方法可以描述为：在N的划分PN和V的划分PV之上,对于cυ∈PV和cn∈PN,求解概率模型p(cυ,cn),进而υ∈PV,n∈PN,p(υ,n)=p(cv,cn).集合的等价类划分有两种方法：①自动聚类：从训练语料中自动学习词语的等价类划分,这种方法得到的划分能够客观地反映真实文本,但是聚类中同样存在数据稀疏问题,而且聚类算法复杂,因此实用性较差［2,3］；②基于义类词典的划分,方法简单,适用性好,但是义类词典是语义分类体系,而VN结构模板不仅与词语的语义特征有关,还与词语的句法特征有关,因此,基于义类词典的划分对于词语结构模板获取是不充分的.［4,5］
　　本文提出了基于动词复杂特征的VN结构模板获取模型,该模型在对集合进行划分的同时考虑了动词的语法特征和语义特征,优于单纯基于义类词典的模型；与自动聚类方法相比较,该模型充分利用复杂特征集的多种信息来限制模型的求解空间,实用性更强.
1　基于复杂特征的VN结构模板获取模型
1.1　问题定义
　　一个动词和名词同现是构成VN结构还是～VN结构,既与动词和名词本身的语法和语义特征有关,也与该同现对的上下文环境的语法和语义特征有关.本文将其本身的特征称为静态特征,将上下文环境的特征称为动态特征.本文主要研究任意动词和特定名词n同现时的结构,在实验中只考虑动词的静态特征,它们有：词性子类SUBV（包括及物动词υt,不及物动词υi等）,音节数SYL（包括单音节mon,双音节bi等）,义类词典《词林》的大类SENSE1(F～J)、中类SENSE2(a～n)和SENSE3小类(01～67)以及动词的词形WORD等.
　　基于复杂特征的VN结构模板获取模型可描述如下：设有特定名词n,动词集合V(n)=｛υ1,υ2,...,υV｝,给定观察数据S=VN+～VN,其中VN=｛(υ,n)|υ和n构成VN结构,υ∈V,n∈N｝,～VN =｛(υ,n)|v和n不构成VN结构,υ∈V,n∈N｝,其中动词v以复杂特征集的形式表示如下：
 
其中fi为特征名,xi为特征值.
　　基于动词复杂特征的VN结构模板获取的中心思想是：①识别与结构相关的特征,并依据这些特征对动词集合V(n)进行划分；②基于动词的划分,估计每个等价类中的动词与n同现构成VN和～VN的概率.其中的关键问题是等价类的划分,对于V(n)的一个划分P,应满足以下条件：
　　①P2V(n)；　　　　②cυ=V(n)；　　　　③cυi,cυj∈P,cυi∩cυj=φ；
　　④cυ∈P,如果｜cυ｜＞1,则υ≠φ,其中∪表示复杂特征集的合一运算.
在该划分上建立的概率模型既可以解释例子集,又可以对未观察的动词和名词同现的结构作出精确的判断.本文利用统计决策树SDT(statistical decision tree)模型［6］进行动词等价类的划分,一方面,SDT的表达能力强于N元模型,相当于插值N元模型；另一方面,SDT模型的最大优势在于自动抽取相关特征的能力.
1.2　用SDT模型生长动词分类树
1.2.1　用SDT表示动词分类树
　　SDT是一个决策机制,它根据一系列特征,赋予每一种可能的选择一个概率值p(f|h),其中h表示一系列特征,f为当前作出的选择,概率值P(f|h) 由前n个特征提问序列q1,q2,...,qn来决定(其中第i个特征提问仅与前i-1个特征提问有关).
　　 图1是用SDT表示的一棵动词分类树,它描述了一个具有某些特征的动词与名词“成绩”同现时构成VN结构或～VN结构的概率.其中,内部结点是提问结点,一个提问结点表示对一个特征的提问,从该结点延伸的树枝代表该特征可能的取值；叶结点是选择结点,表示符合从根结点到该结点的路径上所有“特征―值”的动词与名词“成绩”同现时是构成VN结构还是～VN结构.所提问的特征有：动词的子类SUBV、动词的音节数SYL、动词的义类SENSE等.例如,结点8表示SUBV=υt,SYL=bi且SENSE=Hg的动词与名词“成绩”同现构成VN结构.
 
图1　统计决策树例图 
1.2.2　基于极大似然估计MLE(maximum likelihood estimation)原则的动词分类树的生长算法
　　动词分类树生长算法的关键是每个提问结点所提问的特征的选择问题,本文利用基于信息增益的特征来选择方法［6］生长动词分类树.设X是由任意动词与特定名词n的同现对构成的训练集,X={(υ,n,c,m)｜υ∈V,c∈C},其中C={VN,～VN}是分类集,V是动词集合,m是υ和n的同现次数；设动词特征集为A={A1,...,Ap},特征Ak的取值的集合Vk={υk1,...,υkn},则递归地生长关于特定名词n的动词分类树T的算法描述如下：
　　① 建立动词分类树T的根结点root,将训练集X与root相关联；
　　② 设当前结点为nodei,与nodei关联的训练集为Xi,如果对于任意(υ,n,c,m)∈Xi,有c=VN或c=～VN,则确定为叶结点,返回；
　　③ 对Ai∈A,分别计算
　　　　．熵H(Xi)=-Pclogpc;
　　　　．条件熵H(Xi｜Ak)=-P(c｜Ak=υ)logp(c｜Ak=υ)；
　　④ 计算对Ak提问的信息增益：IG(Ak,Xi)=(H(Xi)－H(Xi｜Ak)),其中IV(Ak)是为了避免选择具有较多取值的特征的倾向所加的系数,表示为IV(Ak)=-nj=1(|Xij|)/(|Xi|)log(|Xij|)/(|Xi|),其中|Xi|是与nodei相关联的训练集Xi中的例子数,|Xij|是训练集Xi中符合条件Ak=υj的例子数；
　　⑤ 确定具有最大信息增益的特征Am=argmaxAkIG(Ak,Xi)；
　　⑥ 依据特征Am的取值的集合Vm={υm1,...,vmn}生长结点nodei的儿子结点nodei1,...,nodein,并将训练集Xi划分为n个子集Xi1,...,Xin,分别将Xi1,...,Xin与nodei1,...,nodein相关联；
　　⑦ 从特征集合A中删除特征Am；
　　⑧ 对于结点nodei1,...,nodein,分别执行②～⑦,进行儿子结点的生长和训练集的划分.
　　这样,一棵基于复杂特征的动词分类树就生成了,所有的叶结点构成动词集合V(n)的最优划分,其中每个叶结点所表示的复杂特征集由从树根到该结点的所有结点的复杂特征组成.
1.3　用最小描述长度MDL(minimum description length)原则搜索动词分类树的最优划分
1.3.1　基于MLE原则的语言获取模型和基于MDL原则的语言获取模型
　　以上讨论的动词分类树的划分和基于划分的概率模型的建造是语言获取的问题.基于MLE原则的语言获取模型为
M=argmaxMp(O｜M). 
其挑选模型的标准是模型与训练数据的拟合性,即模型M要最大可能地解释训练集O,而通常情况下,提供给学习者的数据只是目标语言的一小部分,于是依据MLE原则获取的语法虽然能够很好地解释训练集中的数据,但是对训练集以外的数据的解释能力很弱,这就是语言获取中的过度适合(Overfitting)问题,即对训练数据的不规则性和特异性过分敏感,缺乏归纳能力.而从已知的观察数据归纳出既可以解释已知数据又可以解释未知数据的语法是语言获取中的关键问题.例如,在图1所示的基于MLE原则的生长的SDT中,其最优划分是所有叶结点构成的划分,最优的概率模型是建立在最优划分上的模型,可以看出模型的概括能力很弱,无法判断训练集以外的同现“否定/Hc成绩”的结构.这种过度适合的问题,使得开放测试中VN结构识别的召回率不高.
　　而贝叶斯［7］的语言获取模型为
M=argmaxMp(M｜O)=argmaxM{p(O｜M)×p(M)}. 
与基于MLE原则的语言获取模型相比,贝叶斯语言获取模型除了考虑模型和训练数据的拟合性以外,还考虑了模型M的先验概率p(M).本文依据最小描述长度MDL原则［8］来定义模型的评价函数,即对于给定的观察数据的最好的概率模型是具有最短描述长度的模型,其中描述长度由以下两部分组成：①模型描述长度l(G),即模型的编码长度；②数据描述长度l(O|M),即将模型作为数据的预测时,数据的编码长度.本文将在1.3.2节中具体定义l(M)和l(O|M),这里先定义p(M)=2-l(M).于是P(M)给简单的语法赋予高的概率,这与Occam's Razor的直观意义相符,即简单的语法优于复杂的语法.［7］另一方面,基于MDL原则的语言获取模型又超越了Occam's Razor,即搜索使p(o｜m)×p(m)达到最大的模型M,其中P(M)倾向于简单的模型,而P(O|M)倾向于与训练数据拟合性好的模型.MDL原则就是要在数据拟合程度和模型复杂度之间找到一个鞍点.
1.3.2　基于MDL原则的动词分类树的最优划分算法
　　给定一棵动词分类树,可以得到动词集合V的若干个划分.例如,对于图2的动词分类树,可以得到V的以下划分：
　　.　全集V,
　　．{［SUBV=Vi］,［SUBV=Vt］},
　　
 
图2　动词分类树例图 
因为建立在动词分类树的叶结点组成的划分之上的模型与训练数据的拟合性最好,因此,基于MLE的模型认为这种划分是最优划分；而MDL原则认为,最优划分的评判目标应该是在模型复杂度和数据拟合度上的整体评分最好.
　　对于特定名词n、动词集合V(n)和观察数据S=VN+～VN,设V(n)的候选划分集合Ω={P1,P2,...,PS},其中每个Pi∈Ω(1≤i≤s)都对应一个基于类的概率模型Mi,目标是在候选概率模型中搜索最优的模型.本文用MDL原则构造模型的评价函数,描述如下：
　　① 对于每个候选划分pi={c1,c2,...,cr},构造相应的概率模型Mi={p(υ,n)｜υ∈V(n)},其中υυi,υj∈ck,p(υi,n)=p(υj,n)=p(ck,n),而pVN(ck,n)=∈f(υ,n)/f(υ,n),其中f(υ,n)表示(υ,n)同现的频度.
　　② 计算每个概率模型的数据描述长度：对于任意的动词和名词的同现(υ,n),它的结构可由随机变量X=表示,因此X服从两点分布P{X=x}=.简化上式得P{X=x}=px(1-p)1-x,x=0,1.设x1,x2,...,xn是随机变量X的容量为n的观察样本值,由于样本中Xi(i=1,2,...,n)相互独立,所以观察值x1,x2,...,xn出现的概率是
L=P{X1=x1,X2=x2,...,Xn=xn}=P{Xi=xi}=(pi)xi(1-pi)1-xi. 
定义似然函数为
logL=log(pi)xi(1-pi)1-xi.=xilogpi+(1-xi)log(1-pi), 
本文将-logL作为数据描述长度.
　　③ 计算每个概率模型的模型描述长度：在本问题中模型描述长度由两部分组成,即划分描述长度Lpar和概率描述长度Lpro.设动词集合V(n)的候选划分集合为Ω,则划分描述长度为Lpar(V(n))=log｜Ω｜；概率描述长度的计算方法如下：因为MLE的标准差为O(1/),每个标准差的编码长度为O(｜log(1/)=O(｜-log(1/|)=O(log｜S｜2),于是每个自由参数的编码长度为O(C+log｜S｜／2)=O(log｜S｜／2),本文定义概率描述长度为Lpar=(｜Ω｜／2)×log｜S｜.
　　④ 用MDL原则挑选最优划分和最优模型：Mopt=argminM{Ldat(M)+Lpar(M)+Lpro(M)},因为Mi和Mj,Lpar(Mi)=Lpar(Mj),因此,Mopt=argminM{Ldat(M)+Lpro(M).
　　给定一棵动词分类树T,nodei是其中的一个结点,与结点nodei对应的训练集表示为Xi,将与结点nodei对应的复杂特征集表示为［nodei］,{［nodei］}构成Xi的一个划分,建立在划分{［nodei］}上的概率模型为Mi.基于MDL原则搜索其最优划分opt的递归算法如下：
　　① 将当前结点设为根结点；
　　② 设当前结点为node,如果node是叶结点,则返回［node］；
　　③ 否则,对于node的每个子女结点childi,递归地搜索与它相关联的动词子集Ci的最优划分Pi,令P=Pi,构造基于划分P的概率模型为M,如果模型Mi的描述长度小于模型M的描述长度,即L(Mi)<L(M),则返回划分{［nodei］},否则返回划分P.
　　利用以上的算法可以在一棵动词分类树上搜索到一个动词集合的最优划分.例如,图1所示动词分类树的最优划分为结点a～h构成的划分.
1.4　由动词分类树推导VN结构模板
　　在生长动词分类树并搜索它的最优划分以后,由经过剪枝的动词分类树可以容易地推导出VN结构模板.例如,在图1中,与最优划分对应的结构模板为：

2　基于结构模板的V+N型短语的结构识别
　　基于结构模板的V+N型短语的结构识别可以描述为这样一个问题：给定一个同现(υ,n),其中υ是一个特定的动词,n是一个特定的名词,判断它是VN结构还是～VN结构.可利用的资源有：动词词性词典、动词义类词典、由经过剪枝的动词分类树推导的结构模板.VN结构识别算法描述如下：
　　① 对动词υ标注动词的分类和义类,并建立其复杂特征集的向量表示Q1,...,Qm（因为义类歧义没有完全排除,因此可能有多个向量）；
　　② 分别计算Q1,...,Qm与n同现时构成VN结构的概率pVN(Qi,n)=(Qi,n)+×和构成～VN结构的概率p～VN(Qi,n)=(Qi,n)+×,其中(Qi,n)是以SDT结构模板判断的Qi和n同现时构成VN结构的概率；(Qi,n)是以SDT结构模板判断的Qi和n同现时构成～VN结构的概率；是动词υ在VN结构中出现的概率；是名词n在VN结构中出现的概率；是动词v在～VN结构中出现的概率；是名词n在～VN结构中出现的概率；
　　③ 计算k=argmaxipVN(Qi,n)和l=argmaxip～VN(Qi,n);
　　④ 比较pVN(i)和p～VN(Ql,n),
　　　　．如果pVN(Qk,n)>p～VN(Ql,n),则同现(υ,n)的结构为VN,可信度为pVN(Qk,n)；
　　　　．如果pVN(Qk,n)<p～VN(Ql,n),则同现(υ,n)的结构为～VN,可信度为p～VN(Ql,n).
3　模型分析和测试结果
3.1　模型分析
　　分别从训练集和测试集（训练集和测试集的建造见3.2节）中抽出动词与10个名词“办法”、“标准”、“产品”、“成绩”、“贷款”、“单位”、“干部”、“公司”、“过程”、“合同”同现的数据作为训练集和测试集,测试各种模型的性能.测试指标有：①收敛性：比较模型在不同的训练数据时获得的结构模板的变化数（增加的模板数和减少的模板数之和）,变化数越小,收敛性越好；②模板数：比较模型在不同的训练数据时获得的结构模板数,模板数越小,模型越简单；③精确率：p=×100%；④召回率：r=×100%；其中a是能判断VN/～VN且判断正确的同现次数,b是能判断VN/～VN的同现次数,c是总同现次数.
3.1.1　基于MDL的模型和基于MLE的模型的比较
　　通过比较基于复杂特征和MDL的模型与基于复杂特征和MLE的模型的各种指标,比较基于MDL的模型和基于MLE的模型的性能.结果见图3～6.
　
　图3　MDL和MLE收敛性比较　　　　　　　　　　图4　MDL和MLE模板数比较
　　
　图5　MDL和MLE精确率比较　　　　　　　　　　　图6　MDL和MLE召回率比较
　　通过以上实验可以得出如下结论：① 基于MDL的模型比基于MLE的模型收敛速度快；② 基于MDL的模型比基于MLE的模型简单；③ 虽然基于MDL的模型的精确率比不上基于MLE的模型,但随着训练数据的增加,基于MDL的模型的精确率逐渐逼近基于MLE的模型；④ 基于MDL的模型的召回率优于基于MLE的模型,主要体现在：由于基于MDL的模型的概括能力优于基于MLE的模型,使得该模型对于未观察的同现的处理优于基于MLE的模型,对于标不上义类的同现的处理优于基于MLE的模型.
3.1.2　基于复杂特征的模型和基于义类的模型的比较
　　通过比较基于复杂特征和MDL的模型和基于义类和MDL的模型的各种指标,比较基于复杂特征的模型和基于义类的模型的性能.结果见图7～10.
　　　　
图7　复杂特征模型和义类模型收敛性比较　　图8　复杂特征模型和义类模型模板数比较
　
图9　复杂特征模型和义类模型精确率比较　　图10　复杂特征模型和义类模型召回率比较
　　通过以上实验可以得出如下结论：① 基于复杂特征的模型收敛速度比基于义类的模型稍快；② 基于复杂特征的模型比基于义类的模型简单；③ 基于复杂特征的模型比基于义类模型的精确率高；④ 基于复杂特征的模型的召回率优于基于义类的模型.
3.2　测试结果
　　从70兆字节的新华社语料中抽取(v,n)同现61 324对（238 694次）,涉及5 975个动词和6 568个名词,其中包括VN结构14 081对（38 075次）,涉及2 946个动词和931个名词.对所有的(υ,n)同现标注词性标记、义类标记和音节数标记,从而构成训练集.
　　从训练集中随机抽取3 099对(υ,n)同现,构成封闭测试集,涉及1 095个动词和1 468个名词.从训练集外的10万字的语料中抽取(υ,n)同现3 165对,标注词性标记、义类标记和音节数标记,构成开放测试集,涉及1 173个动词和1 574个名词.
　　利用VN结构模板判断封闭测试集和开放测试集中(υ,n)同现的结构,测试结果如表1所示.
　　表1　　　(%) 

　精确率召回率
封闭测试97.896.3
开放测试94.190.3

3.3　对实验中几个问题的说明
3.3.1　《词林》收词不足的问题
　　在识别阶段,存在着由于《词林》收词不足引起的某些动词没有义类代码的问题.在缺少义类代码的情况下,基于复杂特征的模型可能利用词性信息和音节数信息.例如：动词“造林”的词性标记为υi,音节数为2,但在《词林》中没有义类代码,在识别“造林成绩”的结构时,由2.1节中模板②判断它为VN结构.实验表明,开放测试中,对于动词没有义类的(υ,n)同现,基于复杂特征的模型的识别精确率为85.3%,召回率为66.7%.
3.3.2　义类兼类问题
　　对训练集和测试集中(υ,n)同现数据中的义类兼类动词,虽然利用词性标记进行了排歧,但仍然存在义类兼类问题.本文采取的办法是保留歧义.实验表明,这些义类歧义对于训练和识别的影响都不大.原因是：基于复杂特征和MDL原则的模型得到的结构模板都具有共同的特征.例如,“研究(Gb01/Hg14)成绩”中义类Hg是成组出现的,即“教育(Hg)、训练(Hg)、学习(Hg)、创作(Hg)”,而义类Gb是个别出现的.在开放测试中,对于动词义类兼类的(v,n)同现,基于复杂特征和MDL原则的模型的识别精确率为89.5%,召回率为95.3%；而基于义类和MLE原则的模型的识别精确率为82.5%,召回率为88.4%. 
4　结束语
　　本文提出了基于复杂特征和MDL原则的VN结构模板获取模型.实验表明,在利用结构模板进行VN结构的识别时,这种模型比基于义类和极大似然估计原则的模型具有更高的精确率和召回率.但还有以下两方面的不足：①对于VN结构的识别,除了应该考虑其组成成分的句法和语义特征外,还应该考虑它出现的上下文环境.例如,在句子“这个分析系统性能可靠”中(v,n)同现“分析系统”构成VN结构,而在句子“这个实验用于分析系统的性能”中“分析系统”构成～VN结构.虽然本文考虑的主要是它的内部组成成分,但是基于复杂特征和MDL原则的VN结构模板获取模型是通用的.如果将上下文环境特征加入复杂特征集中,这种模型则可以同时考虑VN结构的上下文环境；②虽然基于复杂特征和MDL原则的模型的鲁棒性优于基于义类和MLE原则的模型,《词林》收词不足的问题仍然是影响VN结构的识别精确率和召回率的主要因素.我们正在尝试用分布相似的方法解决这个问题.
本文研究得到国家自然科学基金资助.
作者介绍：赵军,1967年生,博士生,主要研究领域为自然语言处理,信息检索,语料库语言学.
　　　　　黄昌宁,1937年生,教授,博士生导师,主要研究领域为自然语言处理,信息检索,语料库语言学.
本文通讯联系人：赵军,北京 100084,清华大学计算机科学与技术系
作者单位：赵　军　清华大学计算机科学与技术系　北京　100084
　　　　　黄昌宁　清华大学智能技术与系统国家重点实验室　北京　100084
参考文献
　［1］梅家驹等.同义词词林.上海:上海辞书出版社,1983(Mei Jia-ju et al. Tongyici Cilin. Shanghai: Shanghai Dictionary Press, 1983)
　［2］Brown P F et al.Class-based n-gram Models of natural language. Computational Lingustics, 1992,18(4):467～479
　［3］Dagan I et al. Contextual word similarity and estimation from sparse data. Computer Speech and Language, 1995,9(2):123～152
　［4］Li Hang et al. Clustering words with the MDL principle. In: Proceedings of the 16th International Conference on Com-putational Linguistics. Copenhagen, Denmark: the Association for International Computational Linguistics. 1996
　［5］Pereira F et al. Distributional clustering of English words. In: Proceedings of the 31th Annual Meeting of the Association for Computational Linguistics. Ohio, USA: Association for Computational Linguistics, 1993. 183～190
　［6］Magerman S F. Natural language parsing as statistical pattern recognition ［Ph. D. Dissertation］. Stanford: Stanford University, 1994
　［7］Stanley F Chen. Building probabilistic models for natural language ［Ph. D. Dissertation］. Cambridge, Massachusetts: Harvard University, 1996
　［8］Quinlan J R. Inferring decision trees using the minimum description length principle. Information and Computation, 1989,80(2):227～148
本文1997-08-14收到原稿,1998-01-23收到修改稿
