计算机研究与发展
JOURNAL OF COMPUTER RESEARCH AND DEVELOPMENT
1999　Vol.36　No.7　P.788-793



一种基于信息增益与费用评价函数的特征选择准则
王亚东　郭茂祖　钱国良
摘　要：特征选择问题是机器学习和模式识别中的一个重要问题.然而，在实际应用中，由于没有将特征选择与特征提取过程统一考虑，只注重特征本身的分类性能，没有考虑特征提取的费用问题，导致识别系统的效率较低.文中从实际应用角度，提出一种新的特征选择准则，将特征的分类性能与特征的提取费用统一考虑，利用信息增益与特征提取费用综合评价函数作为特征选择准则，并给出了启发式算法ECFS.将该算法应用于实际领域的学习问题并与决策树算法ID3和BP神经网络进行了比较.实验结果表明，ECFS在保证识别精度的同时，大大减少了特征提取的时间消耗，提高了识别速度.
关键词：信息增益，费用，特征选择，决策树
分类号：TP18
A FEATURE SELECTION CRITERION BASED ON INFORMATION 
GAIN AND COST EVALUATION FUNCTION
WANG Ya-Dong
(Department of Computer Science and Engineering, Harbin Institute of Technology, Harbin 150001)
GUO Mao-Zu
(Department of Computer Science and Engineering, Harbin Institute of Technology, Harbin 150001)
QIAN Guo-Liang
(Department of Computer Science and Engineering, Harbin Institute of Technology, Harbin 150001)
Abstract：Feature selection is an important problem in the fields of machine learning and pattern recognition. However, in real-world domains, the fact that feature selection and feature extraction are not considered together in existing heuristic algorithms leads to the lower efficiency of application system. In this paper, a new feature selection criterion is presented which considers feature selection and feature extraction together. A heuristic algorithm based on information gain and cost of feature extraction evaluation function, ECFS is also given. It is applied to the learning problem in real-world domain and is compared with ID3 and BP algorithms. The experimental results show that under the condition of ensuring the recognition rate, ECFS can reduce a lot of cost of feature extraction and improve recognition speed greatly.
Key words：information gain, cost, feature selection, decision tree▲
1　引言
　　特征选择问题是机器学习和模式识别所面临的一个重要问题［1］.特征选择是指从已知一组特征集中按照某一准则选择出有很好的区分性的特征子集，或按照某一准则对特征的分类性能进行排序，用于分类器的优化设计［2］.目前的特征选择方法主要是传统的模式识别方法，如类内、类间距离度量法［2］，神经网络［3，4］以及一些机器学习方法等［5，6］.这些特征选择准则主要集中讨论特征的可区分性问题，即根据某一准则选择分类性能较好的特征来进行分类，并没有考虑到特征提取的费用问题.然而，在实际应用中，一个特征的优劣仅用其分类性能来衡量是不够的，特征提取的费用也是一项非常重要的考虑因素.因为一般情况下特征提取费用高的特征获得的信息量通常要多于费用低的特征.但是，有时费用低的几个特征获得的信息量总和可能多于一个费用高的特征获得的信息量，并且其费用开销之和小于费用高的特征.因此，确定特征选择准则时考虑每个特征提取的费用是十分必要的.
　　本文从实际应用角度，提出一种基于信息增益和特征提取费用综合评价函数的特征选择准则.将特征的分类性能与特征的提取费用统一考虑，在识别过程中特征的选择与提取同时进行的方法，并给出了启发式算法ECFS.将该算法应用于手写汉字识别系统的特征选择问题，并与决策树学习算法ID3和BP神经网络算法进行了比较.实验结果表明，ECFS在保证识别精度的同时，大大减少了特征提取的时间消耗，提高了识别速度.
2　传统特征选择准则
2.1　距离准则
　　给定一组特征向量集合F={f1, f2,…, fn}，从中选择具有分类能力较好的特征子集，或对特征的分类能力排序，满足类间相似度尽量小而类内相似度尽量大的特性.依据距离准则［2］，应该使不同类别之间的特征均值向量之间的距离最大，而同一类别内的特征均值向量之间的方差和最小.假设各个特征向量之间是统计独立的，根据训练样本，对F中的n个特征向量逐个独立地分析，从中找出m个最好的作为分类特征.例如，给定两类训练样本Wi和Wj，其特征均值向量分别为mi和mj，第k个特征分量为mik和mjk，相应的均值方差为.这里，特征选择的准则函数为.显然，Gk 的值越大，表示第k个特征区分两类Wi和Wj的性能越好.将特征向量f1, f2, …, fn，按照Gk的大小排序，选出能够完全区分出训练样本的特征子集{fi, fj, …, fm}，1≤i＜m≤n.
2.2　神经网络准则
　　神经网络是进行特征选择的一种有效手段.在文献［3］中给出了一种“弱刺激萎缩”学习算法用于手写汉字识别的特征选择.其基本模型和算法构造如下：

　　　　
图1　神经网络基本模型
　　（1） 调整输出节点和输入节点间的连接权重的Perceptron收敛算法；
　　（2） 完成对训练样本集中线性可分样本和线性不可分样本自动分类的“仿生注意”机制；
　　（3） 减少输出节点和输入节点间不必要的联结的“弱刺激缩维”法.
　　由于在输入向量(抽取的手写汉字特征集)中可能包含了很多对于识别过程并无很大的影响的冗余特征，这些特征却占用了大量的网络资源.采用这种学习算法能够使得神经元可以找到输入节点中兴奋度最高、数目最少的一些节点.这些节点所对应的输入特征应当认为是识别所需要的主要特征，从而达到特征选择的目的.
2.3　决策树准则
　　决策树学习算法采用分治策略，在递归构造过程中，在树的结点上利用启发式方法进行特征选择.其中最为著名的是Quinlan的以信息熵作为启发函数的决策树归纳学习算法ID3［5］，被广泛应用于模式识别、专家系统等领域.
　　假设在决策树的第Ti个结点上，含有的训练样本正例数和反例数分别为T+i，T-i,则该结点所具有的信息熵定义如下：

　　如果以特征Xk作为当前结点的分枝属性，Xk具有m个值{V1, V2,…, Vm}，它将Ti分为m个子结点，则有Ti+1={T(i+1)1，T(i+1)2，…，T(i+1)m}.假设第T(i+1)k结点上含有个正例和个反例，则子结点T(i+1)k所具有的信息熵是I(T(i+1)k)，那么以特征Xk为分枝属性得到的子结点集Ti+1所具有的信息熵是：.因此，在第Ti结点上以特征Xk为分枝属性的信息增益是：Gain(Xk)=I(Ti)-I(Ti+1).ID3选择使Gain(Xk)最大的特征Xk作为分枝属性（信息增益最大原则）.整个决策树就是通过递归地利用上述启发式算法选择不同的特征作为树的结点属性建立起来.
　　上述特征选择准则方法主要集中讨论特征的可区分性问题，即根据某一准则选择分类性能较好的特征来进行分类，并没有考虑到特征提取的费用问题.然而，在实际应用中，一个特征的优劣仅用其分类性能来衡量是不够的，特征提取的费用也是一项非常重要的考虑因素.正如医生看病一样，手术开刀可能是最可靠的确认病情方法，然而费用也是非常昂贵的，有些情况完全可以通过其它几种简单的途径来确诊.这在专家系统、故障诊断和一些模式识别领域的实际应用中尤其重要［7］.因此，进行特征选择时考虑每个特征提取的费用是十分必要的.
3　基于信息增益与费用评价函数的特征选择准则
3.1　特征抽取费用
　　特征抽取是获取特征用于分类的一种手段或算法［2］.如何衡量特征抽取的费用问题对于不同的学习领域和实际问题是不同的.给定一组分类特征集合F={F1,F2,F3,…, Fn}，获取特征Fi的费用表示为Cost(Fi).根据实际问题的不同，Cost(Fi)可以用时间、金钱、能量等不同尺度来衡量.对于大多数模式识别问题，如数字识别、汉字识别［8］等，Cost(Fi)的度量大多表现为特征获取的时间或时间复杂度.而在专家系统、故障诊断等领域中［7］，则更多地考虑以金钱费用作为Cost(Fi)的度量单位.
　　为了进一步描述特征提取与费用之间的关系，我们首先对抽取的特征集和对特征抽取的算法集进行编号：F1, F2,…, Fn和A1, A2,…, Am，(其中，m≤n).在这里，算法是一个较为抽象的概念，可以看作是一个程序，也可以是一种操作步骤.对于每一个算法，给出相应的费用函数，从而得到费用函数集合C={C1, C2,…, Cm}.然后建立一个特征抽取控制器（图2），用于建立抽取的特征集与特征抽取算法集的对应关系表.其中每一个特征的抽取过程所对应的算法都用一个费用函数进行估价.特征抽取控制器工作原理：根据给定的不同的特征编号入口参数，激活相应的算法来进行特征抽取，其费用由相应的费用函数来确定.这样，对于每一个特征Fi，可以利用特征抽取控制器，调用相应的算法Aj来获得，并给出其特征提取的费用Cj，即C(Fi)=Cj.

　　　　
图2　特征抽取控制器
3.2　基于信息增益与费用评价函数的特征选择启发式算法ECFS
　　决策树是一种非常有效的归纳学习工具，具有训练（学习）速度、识别速度快的优点，特别适合于处理大规模数据分类问题.在决策树的结点形成过程中，采用的特征选择准则不同，形成不同风格的决策树.其中最为著名的是Quinlan的ID3［5］及其改进算法［6］.然而，由于以往的决策树学习算法在特征选择准则中，没有将特征的选择与提取过程统一考虑，只注重特征的分类性能，没有考虑到特征提取的费用问题.因此，在实际应用中存在效率较低的问题.因为特征提取的费用也是整个系统构造过程中的一项重要考虑因素.所以，进行特征选择时考虑每个特征提取的费用是十分必要的.
　　本文以决策树为工具，将特征选择与特征提取费用统一考虑，建立基于信息增益与费用评价函数的特征选择准则：
　　（1） 信息增益大的特征的优先级高于信息增益小的特征的优先级.
　　（2） 特征提取费用小的特征被选进的优先级高于特征提取费用大的特征.
其中，第(1)条准则是生成的决策树平均高度极小化的优化准则，使得在每一非叶结点进行测试时，能获得关于被测试例子最大的类别信息.从而保证该非叶结点到达各后代叶结点的平均路径最短.使生成的决策树平均深度较小，从而有较快的分类速度.第(2)条准则通过选择那些特征提取费用小的特征，减少整个系统运作的费用，从而达到提高系统效率的目的.
　　基于上述两个准则建立特征选择准则的启发式评价函数：F(A,E) = Gain(A)/(1+α×C(A)).其中Gain(A) 表示选择特征A带来的信息增益，其实际意义与本文第二部分决策树准则中的概念相同；C(A)是抽取特征A的费用，根据不同情况，可以用时间或金钱等度量单位表示；α为调控因子，根据实际情况来确定其取值，以调节信息增益与特征提取费用之间的平衡关系，其值域为(0,1)区间上的实数.
　　从F(A,E)的定义可以看出，F(A,E)表示对于训练例子集E，单位费用内获得的信息量；其取值越大，表明选择的特征越好.当不考虑特征提取的费用时，即C(A)=0或α非常小时，F(A,E)就相当于传统的信息增益启发函数，也即ID3的信息增益准则是本文特征选择准则的一个特例.下面我们给出建立在这种评价准则上的特征选择启发式算法ECFS，算法具体描述如下：
　　算法1. 给定训练例子集E，例子的特征集合A={A1, A2,…, An}，F(Ai,E)是上面定义的评价函数：
　　PROCEDURE ECFS(E)
　　BEGIN
　　　　IF　E中只含有正例或反例，则返回叶结点，并标记类别；
　　　　ELSE 
　　　　　　选择一个特征Ak，Ak∈A，使得F(Ak,E)最大；
　　　　　　For　Ak的每个取值Vj，递归构造子树ECFS(Ej)；Ej中只包含Ak取值为Vj的例子；
　　　　　　返回非叶结点及相应的特征Ak；
　　　　ENDIF
　　END
3.3　特征抽取控制器引入ECFS
　　为了进一步将特征选择与特征提取过程紧密结合起来，我们在特征选择的训练过程中将选择的特征编号引入ECFS，然后在识别过程中利用特征抽取控制器减少特征提取的数量，从而达到提高系统效率的目的.具体方法如下：
　　(1)训练过程.首先选择训练样本作为训练例子集合，然后利用特征提取算法抽取每个样本的所有特征，构造一个具有n维的特征向量空间（训练例子特征集）.然后利用启发式算法ECFS，建立特征选择的决策树，在决策树的每个非叶子结点上，引进一个新的结点信息：该结点上要用选择的特征编号；
(2)识别过程.在用决策树进行识别的时候，给定一个新的待识别样本，根据树结点的特征编号信息，通过特征抽取控制器，激活相应的算法来抽取与之对应的特征.这样，通过特征抽取控制器，ECFS可以做到边分类边进行特征的选择和抽取，当需要某个特征进行分类的时候，再进行该特征的抽取.而不必抽取全部特征，从而减少了特征抽取的数量，提高了系统效率（因为系统的许多费用大都消耗在特征提取上）.
3.4　一个特征选择的例子
　　下面我们构造一个简单的特征选择例子来说明ECFS的运作机制（见表1）.
表1　一个特征选择的例子

例子x1x2类
e110P
e201N
e310P

特征费用
x15
x28


　　在表1的例子中，共有4个例子，分为两类.用两个特征x1,x2进行分类，x1和x2的特征提取费用分别为5和8.如果采用决策树算法ID3［5］，利用x1或x2都可以将训练例子集完全区分开.其信息增益均为2.结果为{x1=1→e∈P; x1=0→e∈N;}或{x2=0→e∈P; x2=1→e∈N;}.也就是说，ID3只考察特征的分类性能，无法区分x1和x2的实际应用效率.而如果利用ECFS，则只选择特征x1作为分类特征.因为，假设α取值为0.5，则F(x1,E)=0.57； F(x2,E)=0.4；显然，F(x1,E)＞ F(x2,E)，故ECFS只选择特征x1作为分类特征，结果为{x1=1→e∈P; x1=0→e∈N;}.这样，在识别一个新的例子时，只需要获取特征x1的取值，就可以通过ECFS判别其所属类别，从而节省了费用.通过这个简单的例子可以看出，将特征选择和特征提取统一处理的ECFS算法的优点.
　　利用特征选择准则启发式算法ECFS和特征抽取控制器，将特征的分类性能与特征提取的费用统一考虑，相互平衡，相互联系，使得整个系统形成一个有机的整体. 
4　实验结果
　　本文将ECFS算法应用于实际应用领域的手写汉字识别系统，并与传统的决策树学习算法ID3和神经网络BP算法进行比较.我们选用1000个常用汉字，由40个学生来提供手写汉字样本.每个汉字包含20个不同的样本，将每个字的前12个样本作为训练样本集（共12000个例子），将其余8个样本作为识别集（共8000个例子）.抽取了横竖笔划特征、周边特征、结构划分特征和特征点作为分类特征.抽取的特征集构成了一个12维的特征空间：A={A1, A2, A3,…, A12} (见表2).
表2　抽取的手写汉字特征集

A1笔划个数 (1-z)A7下周边特征(0-f)
A2长横的位置(0-2)A8结构划分特征(0-2)
A3长竖的位置 (0-2)A9左上区的特征点个数(0-4)
A4左周边特征(0-f)A10右下区的特征点个数(0-4)
A5右周边特征(0-f)A11左下区的特征点个数(0-4)
A6上周边特征(0-f)A12右上区的特征点个数(0-4)


　　这里，本文采用时间来定义特征抽取算法的费用.现将启发函数F(A,E)在手写汉字识别中的应用意义说明如下：F(A,E)=Gain(A)/(1+α×C(A))，其中Gain(A) 是选择特征A带来的信息增益，C(A)表示抽取特征A的实际时间消耗，α为调控因子，在本文试验中取值为0.45；F(A,E)表示对于训练例子集E，单位时间内获得的信息量.在486微机的实验结果比较如表3所示. 
表3　ECFS与ID3、BP算法的比较结果

　BP算法ID3ECFS
训练时间260 h2 h2 h
识别速度14 字/s12 字/s21 字/s
识别精度77.85%76.32%75.24%


　　从表3的试验结果可以发现，利用ECFS算法构造的特征选择决策树，在识别过程中特征提取与识别交替进行的方法（只提取识别所需要的特征）是非常有效的.ECFS继承了ID3算法学习、识别速度快的优点，在识别率上与ID3相差不多的同时，识别速度有很大的提高，提高了系统效率.
　　另外，我们还发现，ECFS训练后的决策树平均树高只有8（最大高度为12）.也就是说，在识别的过程中，对于每个要识别的汉字，所需特征平均只有8个即可,减少了特征提取的数量,提高了识别速度.同时，ECFS对于不同的汉字，所使用的特征有较大不同.其中，对于复杂的汉字，周边特征使用的频率很高（87%）；而对于一般字，结构特征和分区特征点的使用频率较高（82%）；在简单汉字的识别上，笔划特征使用相对要多一些（78%）.可以看出，ECFS在识别不同复杂度的汉字时，能够有效地选择不同的特征. 
5　结论
　　本文从实际应用角度，将特征的分类性能与特征的提取费用统一考虑，提出一种基于信息增益和特征提取费用综合评价函数的特征选择准则，在识别过程中特征的选择与提取同时进行的方法，并给出了启发式算法ECFS.将该算法应用于手写汉字识别系统的特征选择问题.实验结果表明，ECFS在保证识别精度的同时，大大减少了特征提取的时间消耗，提高了识别速度.■
基金项目：本课题得到国家自然科学基金、国际合作项目彩色匹配、哈工大校管基金资助.
作者简介：王亚东，男，1964年生，副教授，主要研究领域为专家系统、彩色匹配.
　　　　　郭茂祖，男，1966年生，副教授，博士，主要研究领域为机器学习、彩色匹配、非　　　　　　数值并行算法.
　　　　　钱国良，男，1971年生，博士生，主要研究领域为机器学习、彩色匹配、模式识别.
作者单位：王亚东(哈尔滨工业大学计算机科学与工程系　哈尔滨　150001)
　　　　　郭茂祖(哈尔滨工业大学计算机科学与工程系　哈尔滨　150001)
　　　　　钱国良(哈尔滨工业大学计算机科学与工程系　哈尔滨　150001)
参考文献：
［1］陈彬，洪家荣，王亚东.最优特征子集选择问题，计算机学报，1997, 20(2): 133～138
　　　(Chen Bin, Hong Jiarong, Wang Yadong. The problem of finding optimal subset of features. Chinese Journal of Computers(in Chinese), 1997, 20(2): 133～138)
［2］蔡元龙. 模式识别. 西安: 西安电子科技大学出版社，1992, 104～120
　　　(Cai Yuanlong. Pattern Recognition(in Chinese). Xi'an: Xi'an University of Electronic Science and Technology Press. 1992, 104～120)
［3］刘迎建，戴汝为，张立清.基于神经网络的手写汉字特征选择，模式识别与人工智能，1992, 5(1): 37～43
　　　(Liu Yingjian, Dai Ruwei, Zhang Liqing. Feature selection of handwritten Chinese character based on neural network. Pattern Recognition and Artificial Intelligence(in Chinese). 1992, 5(1): 37～43)
［4］权光日，崔明根，张朝晖，洪家荣.基于Hopfield-Tank模型的变参数神经网络方法，电子学报， 1996, 24(8): 78～81
　　　(Quan Guangri, Cui Minggen, Zhang Zhaohui, Hong Jiarong. A variant parameter network method based on Hopfield-Tank model. Acta Electronica Sinica. 1996, 24(8): 78～81)
［5］Quinlan J R. Induction of decision trees. Machine Learning, 1986, 1(1): 81～106
［6］洪家荣，丁明峰，李星原，王丽薇.一种新的决策树归纳学习算法，计算机学报，1995, 18(6): 18～21
　　　(Hong Jiarong, Ding Mingfeng, Li Xingyuan, Wang Liwei. A new algorithm of decision tree induction. Chinese Journal of Computers(in Chinese), 1995, 18(6): 18～21)
［7］Pazzani M, Merz C, Murphy P et al. Reducing misclassification costs. In: Proceedings of the 11th International Conference of Machine Learning, New Brunswick: Morgan Kaufmann, 1994, 217～225
［8］钱国良.基于机器学习的手写汉字识别的研究与实现［硕士论文］，哈尔滨工业大学，哈尔滨，1995
　　　(Qian Guoliang. Research and implementation of handwritten Chinese character recognition based on machine learning［master dissertation］(in Chinese). Harbin Institute of Technology, Harbin 1995)
收稿日期：1998-10-06
修稿日期：1999-04-06
