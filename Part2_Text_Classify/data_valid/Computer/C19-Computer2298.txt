计算机研究与发展
JOURNAL OF COMPUTER RESEARCH AND DEVELOPMENT
1999年 第36卷 第5期 Vol.36 No.5 1999



B样条神经网络的构造理论
於东军　王士同
摘　要　文中首先讨论了B样条基函数的特性，在此基础上采用构造性的方法从理论上证明了B样条神经网络能够以任意精度逼近任意定义在致密区间上的连续实函数.最后给出了构造性算法，使用此算法，能在满足误差要求的条件下，构造出几乎最小的B样条基函数.
关键词　B样条，神经网络，构造理论
中图法分类号　TP18
CONSTRUCTIVE THEORY FOR B-SPLINE NEURAL NETWORK
YU Dong-Jun and WANG Shi-Tong
(Department of Computer Science,East China Shipbuilding Institute,Zhenjiang　212003)
Abstract　The characteristics of B-spline basis function is first discussed, based on which B-spline neural network is proved to be a universal approximator. And then a constructive algorithm is presented. It is proved that this algorithm can be used to build a B-spline neural network with minimum hidden units to approximate any continuous function defined on compact set to a prescribed accuracy.
Key words　B-spline, neural network, constructive theory
1　引　　言
　　对于一个给定的非线性函数，我们都知道用神经网络可以以任意的精度来逼近它，原因就是神经网络是全局逼近器［1］.最常使用的神经网络就是3层网.对于3层神经网络而言，输入至输出的映射实际上是通过基函数的加权组合来实现的，当基函数取不同时，网络有不同的性质，并且计算复杂程度不同.常用的基函数有Sigmoid、高斯型等等，当基函数取B样条时，称此神经网络为B样条神经网络.
　　对于如图1所示的3层神经网络，输入至输出的映射是通过基函数αi()的加权组合来实现的，即y=，其中=(x1,x2,…,xn)T.将基函数取为B样条函数时，称此3层神经网络为B样条神经网络.由于B样条函数本身具有极好的性质［2］（正定性，归一性，致密性），因此B样条神经网络得到了极为广泛的应用.


图1　B样条神经网络的拓扑结构
　　虽然B样条神经网络有着很多优点，但是当面对一个实际应用问题时，隐节点数目的选取却是个问题，至今未有理论来指导.本文就是旨在解决这个问题，给出找到几乎最小隐节点数目的算法.
2　B样条基函数简介
　　B样条函数是工程上常用的一种插值函数，由于它符合工程实际需要，因此应用范围很广.
　　假定输入向量为：=(x1,x2,…,xn)T，xi∈Ui，Ui为有限区间，可以定义为：
Ui={xi|xmini≤xi≤xmaxi}
　　对第i维输入区间进行分割：
xmini<ρi,1<ρi,2<…<ρi,mi<xmaxi
ρi,j称为xi的第j个内插点.
…<ρi,-1<ρi,0=xmini; xmaxi=ρi,mi+1<ρi,mi+2<…
称为外插点.
　　（1） 单变量基函数
　　设输入域为U=［xmin,xmax］，在此区间内有m个内插点，并在区间之外的两边分别定义k-1个外插点.k为B样条函数的阶数.这样可定义单变量基函数为：
αj(x)=Nkj(x)
其中：

　　图2表示了k=3时的B样条基函数.


图2　单变量3阶B样条基函数
这样定义的B样条函数有3条重要性质［2］：
　　① 正定性：Nkj(x)>0，对于x∈［ρj-k,ρj),1≤j≤m+k.
　　② 紧密性：Nkj(x)=0，对于x［ρj-k,ρj)，1≤j≤m+k.
　　③ 归一性：，对于任意的x∈U.
　　我们把集合α={Nkj(x),j=1,2,…,m+k}称为k阶B样条基函数集.
　　(2) 多变量基函数
　　输入向量为=(x1,x2,…,xn)T∈Rn，定义基函数，Nkii,ji(xi)表示xi(i=1,2,…,n)的第ji(ji=1,2,…,mi+ki)个单变量基函数，且单变量xi的基函数阶数为ki.则多变量基函数的总数为，且αj1j2…jn()同样满足正定性，紧密性，归一性.
　　① 正定性：αj1j2…jn()>0,对于　i,xi∈［ρi,ji-mi,ρi,ji),1≤ji≤mi+ki.
　　② 紧密性：αj1j2…jn()=0,对于　i,xi∈［ρi,ji-mi,ρi,ji),1≤ji≤mi+ki.
　　③ 归一性：，对于任意的 =(x1,x2,…,xn)T∈U1×U2×…×Un.
　　其证明见文献［2］.
3　B样条神经网络的构造理论
　　在本节中，我们将给出一个定理，该定理表明：可以通过构造的方法得到一个B样条神经网络，此网络能以给定的精度逼近任意给定的定义在致密区间上的多变量连续实函数.
　　首先，我们引入几个定义［4］.设UR为定义域，A(x)为定义在U上的函数.存在x*∈U，使得对任意的x∈U,有A(x)≤A(x*)，称A(x)为峰值函数，并称x*为峰值点.
　　定义1. 若集合α={Ai(x),i=1,2,…,m}中的每个Ai(x)均为峰值函数，且对于任意的x∈U，有成立.称α为峰值基函数集.
　　显然单变量的B样条基函数集是满足定义1的.
　　定义2. 对于给定的函数f:U→R以及一个峰值基函数集α={Ai(x),i=1,2,…,m}，我们定义函数:U→R为，其中x*i为Ai(x)的峰值点.称为在α上对f的插值.
　　这样，当Ai(x)取B样条函数时，称为在α上对f的B样条插值.这时就对应于一个B样条神经网络，见图3.


图3　单输入的B样条神经网络
　　定义3. 设αi(xi)={Aij(xi),j=1,2,…,mi}为定义在UiR上的峰值基函数集，i=1,2,…,n，那么定义={A1i1(x1)A2i2(x2)…Anin(xn),1≤ij≤mi,1≤j≤n,xi∈Ui},当有… 成立，则称其为定义在U1×U2×…×Un上的乘积峰值基函数集.
　　由第2节的说明，多变量B样条基函数符合定义3，我们称之为B样条乘积峰值基函数集.
　　现在，可以给出定理1.
　　定理1. 设f:→R为定义在致密区间Rn上的连续实函数，则对于ε>0，存在上的一个B样条乘积峰值基函数集Θ={A1i1(x1)A2i2(x2)…Anin(xn)}，使得成立，其中()为在Θ上对f的B样条插值.
　　证明. 为了标记上的简便，我们假定输入的维数为2，且=［0,1］2，各维上的B样条基函数的阶数为3.
　　由于f在上是连续的，且是致密的，则f必定一致连续.因此对于ε>0，必定存在δ>0，使得当时，有成立，其中(x1,y1),(x2,y2)∈.
　　我们先沿x轴将区间[0,1]均匀划分N个等份（内插点，并在区间两边各找相应的两个外插点（因为阶数为3），当N足够大时，令.这样就可以沿x轴定义B样条基函数集α={Aj(x),j=1,2,…,N+2}，其中Aj(x)=N3j(x)，见图4所示.


图4　沿x轴定义B样条基函数
　　　同理，可以沿y轴定义B样条基函数集β={Bj(y),j=1,2,…,N+2}，且Bj(y)=Aj(y),j=1,2,…,N+2.然后可以构造乘积B样条基函数
Θ={Cij(x,y)|Cij(x,y)=Ai(x)Bj(x),i,j=1,2,…,N+2}
　　依据定义2，并注意到Ai(x)的峰值点为，则在Θ上对f的B样条插值可以写为：(x,y)=.
　　对于任意给定的一点(x,y)∈，我们总可以将其定位于：

其中0≤i,j≤N-1.
　　又因Ai(x),Bj(y)均为3阶B样条函数，即每一维上至多有3个基函数不为零，这样上式可以重写为：









其中，利用B样条函数的归一性有
a1+a2+…+a9=［Ai+1(x)+Ai+2(x)+Ai+3(x)］．［Bi+1(x)+Bi+2(x)+Bi+3(x)］=1．1=1.
　　不失一般性，不妨设分别为上式中的最大最小值，就有下式成立：
且　　　　　
结合上式就有：

即


由于点和点之间的距离≤δ，
所以

即

由于点(x,y)与点之间的距离亦≤δ，就有：

综合上面两式，就有：

　　结合点(x,y)的任意性，就证明了定理1.
　　定理1从理论上能够保证，对于任意定义在致密区间上的连续实函数，存在B样条基函数，用这些基函数实现的B样条神经网络能以任意给定的精度逼近原函数，但并没有给出如何求得基函数的方法，这样在实际应用很容易导致基函数过多.本文下面给出一个算法，使用该算法，在满足允许误差的条件下找出几乎最小的基函数集.
4　构造算法
　　同样，为了简明起见，这里的算法描述的是二维情形.
　　设f:为连续实函数，给定一个允许误差ε，我们可以找到一个δ，使得当时，有.基于δ，我们设置　，这里(x≥0)表示不大于x的最大整数.基于N，可将划分为N2个网格.
　　算法1. 初始化：基于上述网格划分，我们初始化所谓的极值集合S0：
　　FOR 0<i,j<N
　　　{
　　　　若
　　　　或 
　　　　成立，则
　　　　
　　　　　}
　　我们用t表示迭代次数，Φt(x,y)表示在第t次迭代时的误差函数.初始时，设置t=0，Φ0(x,y)=f(x,y).
　　步骤1. t=t+1,并引入


假定Stx，Sty分别包含有p,q个元素，那么总可以将这些元素排序为：
　　　　　　　　Stx中：0≤x1<x2<…<xp≤1,(1≤p≤N-1)
　　　　　　　　Sty中：0≤y1<y2<…<yq≤1,(1≤q≤N-1)
　　对于Stx，我们在区间[0，1]两边分别定义两个适当的外插点x-2,x-1和xp+2,xp+3(例如可取,，)，0作为x0，1作为xp+1.使用这些插点，就可以沿着x轴定义3阶B样条基函数：
αt={Ati(x)|Ati(x)=N3i(x),1≤i≤p+3}
上式中N3i(x)是按下式递归定义的：

其中1≤i≤p+3.
同理，可以沿着y轴定义3阶B样条基函数：
βt={Btj(y)|Btj(y)=N3j(y),1≤j≤q+3}
上式中的N3j(y)同样是递归定义的：

其中1≤j≤q+3.
　　步骤2. 利用αt,βt,为Φt(x,y)构造插值t：

xi,yj分别为Ati(x),Btj(y)的峰值点.
设置Φt+1(x,y)=Φt(x,y)-t(x,y)，并按下述方法更新St：
FOR 0≤i,j≤N，　　　　　　　(1)
　　AND 
　　　　　　　　　　　　　
　　　　　　　　　　　　(2)
　　OR
　　　　　　　　　　　(3)
　　　　　 　
　　　　　　 St∪Et+1→St+1;
　　　　　 　}
　　步骤3. IF Et+1为空集THEN 
　　(x,y)=0(x,y)+1(x,y)+…+t(x,y)
结束算法.
ELSE 转步骤1.
　　下面我们就是要来证明，按照上述算法构造出来的B样条神经网络满足给定的误差要求ε，也就是仍然要满足定理1的结论.
　　定理2. 按照上述算法构造出来的B样条神经网络仍然满足定理1的结论.
　　证明. 由算法的步骤3可以知道，如果存在点满足式(1)，必定满足式(2)或式(3)，这样算法才能继续.否则，Et+1将为空集，这也就意味着对于任意的点∈，均满足：

　　又由于
　　这也就是说下式成立：

　　对于任意给定的点(x,y)∈，我们总可以确定其范围：

　　这样，当N充分大时，必定有：

　　　　　　　　　　　　　证毕.
5　结　　论
　　本文采用了构造性的方法证明了B样条神经网络仍然是全局逼近器，并且给出构造几乎最小隐节点数目的算法，这就从理论上为B样条神经网络的使用提供了依据，具有明显的实际应用意义.
本课题得到国家自然科学基金资助(项目编号 69673001).
作者简介：於东军，男，1974年10月生，硕士研究生，研究方向为神经网络、人工智能.王士同，男，1964年10月生，教授，英国Bristol大学博士后，主要研究方向为人工智能、神经网络、模糊数学.
作者单位：华东船舶工业学院计算机科学系　镇江　212003
参考文献
　1　张立明.人工神经网络的模型及其应用.上海：复旦大学出版社，1993
(Zhang Liming. Models and Applications of Artificial Neural Networks(in Chinese). Shanghai:Fudan University Press,1993)
　2　孙增圻，张再兴，邓志东.智能控制理论与技术.北京：清华大学出版社，1991
(Sun Zengqi,Zhang Zaixing,Deng Zhidong.Intelligent control theory and its applications.Peking:Tsinghua University press,1997)
　3　Lin C T. Fuzzy Neural Networks. England: Prentice-Hall Press, 1997
　4　Wang Peizhuang et al.Constructive theory for fuzzy systems.Fuzzy Sets and Systems,1997,88(2):180～185
　5　胡守仁等.神经网络导论.长沙：国防科技大学出版社，1993
(Hu Shouren et al. Introduction to Neural Networks(in Chinese). Changsha:National University of Defence Technology Press, 1993)
原稿收到日期：1998-07-16
修改稿收到日期：1998-12-04
