计算机应用研究
APPLICATION RESEARCH OF COMPUTERS
2000　Vol.17　No.2　P.34-37



神经网络规则抽取研究
孙晨　周志华　陈兆乾
摘 要 尽管神经网络已经在很广泛的领域得到应用，但由于训练好的神经网络中的知识不易于理解，神经网络被视为一个典型的谙结构。从神经网络中抽取规则来表示其中隐含的知识是解决这个问题的一个有效的手段。将对一些具有代表性的神经网络规则抽取算法进行综述分析，并提出一些未来的研究重点。
关键词 神经网络 规则抽取 机器学习
1 引言
　　神经网络技术自从80年代复苏以来获得了较大的发展，由于其具有强有力的非线性处理能力，可以在数据量较大、领域信息不完备以及存在噪音数据的情况下取得传统符号学习方法所难以达到的效果。但是，由于训练好的神经网络模型是典型的谙结构，获取的知识不易被人理解，使其在数据挖掘、知识精化等方面的应用中受到了很大的限制。为了充分发挥神经网络技术的特长，获取神经网络中的知识已经成为该领域中的一个非常重要的研究课题。
　　获取神经网络模型中的知识主要有两大研究分支：一是研究如何利用神经网络来精化规则；二是研究从神经网络中抽取规则。本文主要讨论后者，在分析现有的典型规则抽取算法基础上，对神经网络规则抽取技术未来的研究发展提出一些看法。
2 规则抽取研究的重要性
　　神经网络技术从四十年代问世以来，已经成功地应用在很多领域，但是其能力仍受到怀疑。主要原因在于神经网络通过训练学习到的知识以数值的形式存储在网络中，由于其知识表示的分布式特性，使用者难以对神经网络的能力有清晰的了解，也无法获得明确的推理过程的解释。由于对知识表示的可理解性是学习算法优劣的重要评价标准之一[1]，因此规则抽取对神经网络学习的研究有重要的意义。从神经网络中抽取规则的目的是将神经网络中隐含的知识以一种易于理解的方式明确地表达出来。一方面，以规则的形式表示神经网络获取的知识可以方便使用者和设计者理解神经网络的推理过程；另一方面，通过抽取出来的规则，用户可以发现输入数据中一些以前被忽略的重要关系，从而帮助用户进行决策处理。此外，除了可以作为训练好的神经网络的知识解释器，神经网络规则抽取技术作为一种适用于各种不同领域的规则学习的工具，其有效性和正确性也已经得到证实。
　　正因为神经网络规则抽取具有上述重要意义，这方面的工作受到了越来越多的关注。IEEE Transaction on Neural Networks在1998年底专门出版了一期专刊，Tickle等在首篇文章中明确提出从神经网络中抽取规则是当前神经网络界急需解决的问题[2]，这说明了对规则抽取的研究目前已经成为一个重要的研究方向。
3 规则抽取算法
　　神经网络规则抽取的研究最早开始于80年代末。Gallant在1988年[3]描述了一个可以用IF-THEN规则解释推理结论的神经网络专家系统，自此以后，很多研究者都进行了规则抽取方面的研究工作，并取得了大量的成果。根据抽取规则算法设计思想的不同可以分成基于结构分析的算法和基于性能分析的算法两大类。根据抽取规则算法适用的神经网络结构不同，可以分成针对特殊神经网络结构的算法和通用型规则抽取算法。根据抽取的规则形式不同又可分为抽取最普通的IF-THEN规则的算法、抽取模糊规则的算法、抽取决策树规则算法以及抽取区间规则的算法等。在本节中，我们从规则抽取算法的设计思想出发，分别对基于结构分析的和基于性能分析的典型算法进行介绍和分析。
3.1 基于结构分析的规则抽取算法
　　基于结构分析的神经网络规则抽取算法把规则抽取看成一个搜索过程。其基本思想是把已训练好的神经网络结构映射成对应的规则。图1就表示一个最简单的规则抽取算法。

图1　最简单的规则抽取
　　由于搜索过程的计算复杂度和输入分量之间是指数级的关系，当输入分量很多时，会出现组合爆炸问题。因此，此类算法一般采用剪枝聚类等方法减少神经网络中的连接以降低计算复杂度。比较典型的算法有以下几种：
　　(1)Gallant方法[3]
　　随着神经网络研究的兴起，于80年代末， Gallant首先提出了一个新型的神经网络专家系统，并描述了一个简单的规则抽取算法用于解释该专家系统所做的推理。
　　该算法抽取单个规则来解释神经网络如何为某个给定事例(Case)得出结论。因此，其基本思想就是从当前已知的信息集中选择一个能有效地产生该结论的最小信息集合。也就是说不管其它未知的输入分量的取值为多少，只要满足该最小信息集合的取值要求就可以得出该结论。
　　严格地说，该算法并不是一个真正意义上的规则抽取算法。其主要目的不是进行规则抽取，而是希望对一个已知结论的推理给出一个合理的解释，因此算法简单，只适用于小型的神经网络。
　　(2) Subset算法[4]
　　由Fu提出的Subset 算法是最早的神经网络规则抽取算法之一。基于搜索的规则抽取算法大多是在它的基础上发展起来的。
　　Subset算法采用简单的广度优先算法抽取普通的IF-THEN规则。Subset是由神经网络中所有连接组成的集合的幂集的一个子集。算法从由单个连接组成的初始Subset集合开始测试，检测其每个成员能否有效地保证超过偏移值，如果可以的话则用规则表述出来。不断扩大Subset的大小并用同样的方法测试产生规则，直至所有可能的Subset都被搜索过。最后删除冗余的规则。
　　Subset算法没有采用降低计算复杂性的技术，因此它只适合处理小型的问题。Saito和Nakano[5]提出的KT算法，用限定规则前件数目的方法对Subset算法进行了改进，并将属性分为“正属性”(Pos-atts)和“负属性”(Neg-atts)，对其分别进行搜索。这样，虽然降低了搜索的复杂性但也限制了抽取规则的能力，无法对原神经网络进行精确地描述。
　　(3) MOFN算法[4]
　　MOFN算法是基于Towell 和Shavlik提出的KBANN(Knowledge based networks)神经网络的一种规则抽取算法(如图2所示)。

图2　抽取MOFN规则
　　其抽取的规则形式为M-of-N的规则：
if (M of N antecedents are true) then ...........
　　MOFN算法技术的关键在于引入等价类(Equivalence class)的概念。算法首先使用标准的聚类算法将神经网络中权值接近的连接合并创建等价类，并将每个等价类的权值设成该组的平均值，然后去掉那些对结果影响不大的等价类，在不调整权值的前提下对神经网络重新进行训练，最后直接根据网络结构和权值抽取出规则，并将规则简化。
　　由于使用一般的神经网络算法训练好的神经网络中的连接权大多分散地分布在权值空间中，难以对其进行处理，因此MOFN算法仅适用于KBANN神经网络。为了扩展其适用范围，Craven在1993年提出用柔性权共享(Soft weight-sharing)的方法训练神经网络[1]，在训练中鼓励权值聚类，然后再对用该方法训练好的神经网络使用MOFN算法，可以有效地抽取MOFN规则。
　　MOFN算法抽取出来的规则集比较简单易懂，而且这种规则形式也减少了产生的规则数。另外，由于采用了等价类的方法对连接进行了聚类，也使得最后抽取规则的时间花费大大减少。
　　MOFN算法的主要缺陷在于：首先，该算法要求神经网络中的每个神经元的激活值为双极值模式，这就对激活函数的选择给出了某些限定，从而限制了神经网络的功能。其次，该算法要求输入属性的取值为离散值，这就使得神经网络学习系统在处理连续属性方面的优势难以得到发挥；第三，该算法要求每个神经元表示一个唯一的概念，这与神经网络中通常的分布式知识表示有较大的冲突，极大地限制了其适用范围。
　　(4) RX(Rule-extraction)算法[6]
　　该算法是Setiono提出的一个适用于标准的三层前馈网络的通用型规则抽取算法。该算法抽取的是普通的IF-THEN规则。
　　该算法在最基本的神经网络规则抽取算法上作了两方面的改进：(1)将隐含层神经元的激活值通过聚类离散化，也就是说用一组离散值来表示每个隐含层神经元的激活值；(2)采用对隐层神经元再划分的技术。
　　该方法对于每个输出层神经元，直接根据与其相连接的隐层神经元的离散化的激活值构造相应的规则。对于隐层神经元，首先考虑与其相连接的输入神经元的个数，如果与之相连的输入神经元的数目不超过标准，则直接对应产生描述规则。否则，将生成一个子网络，其中输出结点的个数为该隐层神经元的离散激活值的数目，输入结点的个数为与该隐层神经元相连的输入神经元的个数。对子神经网络进行训练后再调用本算法抽取规则。显然，RX算法以递归的处理方式进行规则抽取。
　　另外，张朝辉[12]等人提出了一种类似于RX算法的用神经网络发现分类规则的算法，其特点是采用了遗传算法(GA)对神经网络结构进行剪枝。
　　在大部分基于搜索的神经网络规则抽取算法中为了减少搜索的时间而不得不限制规则前件的个数，该算法采取对隐含层神经元进行再构造的方法，虽然增加了算法的训练时间，但对神经网络的描述相对更为精确。
　　该算法适用于标准的三层前馈神经网络。为了能有效地产生规则，算法要求对神经网络先进行剪枝以去掉对结论影响不大的连接。由于在规则抽取过程中可能递归地生成多个子网络，因此该算法的时间开销很大。由于对激活值进行了离散化，与以前的算法相比RX算法对激活值的范围减少了限制，但该算法仍要求输入属性为离散属性，要处理连续属性仍需要将其先离散化[7]。
3.2 基于性能分析的规则抽取算法
　　与基于结构分析的算法不同，基于性能分析的神经网络规则抽取算法不把规则抽取过程看作一个对网络结构进行搜索的过程，而是把神经网络看成一个整体来处理，在规则抽取过程中不考虑具体的神经网络结构。
　　(1) RF(Rule from facts)算法和RN(Rule from networks)算法[8]
　　Saito和Nakano提出这两种算法用于抽取DNF(Disjuctive Normal Form)形式的规则。RF算法是使用启发式搜索，对搜索空间剪枝后从事实中直接抽取规则。RN算法则从训练好的神经网络中抽取规则。RN算法首先根据一个真事实(Positive fact)通过调整输入属性的取值范围，极大化一条规则，然后根据伪事实(Negative fact)来缩小该规则，直至该规则不包含任何反例为止。
　　该算法是最早的神经网络规则抽取算法之一。它把整个网络看成一个整体，不考虑网络的具体结构，主要通过事实来抽取规则。
　　(2) 基于学习的规则抽取算法
　　基于学习的神经网络规则抽取算法将规则抽取视为一个归纳学习问题。Craven 和Shavlik 根据该思想提出了TREPAN等算法来抽取多种形式的规则[9,10]。根据Craven 的定义，规则抽取是给定一个训练好的神经网络以及用于对其进行训练的数据，产生一个可理解的概念描述，该描述将以与网络相同的方式对示例进行划分。
　　其关键技术在于把规则抽取过程视作一个由数据和提问驱动的学习过程。算法的基本思想是提供了两个调用来回答提问：一个是实例调用(Example Oracle)，它用于随机地产生实例，并根据这个实例产生若干条假设规则。简单地说，对于一个实例，直接将其对应成一条最特殊的规则，这也就是最基本的第一条假设规则，在此基础上，任意删除一个或多个属性就可以产生其它的假设规则，删除的前件越多，规则的泛化能力越强，覆盖的实例的分类错误性也越大，这就需要对规则进行测试。另一个是子调用(Subset Oracle)，它的设计随学习模型的不同而不同，其主要功能就是根据已经训练好的学习模型测试给出的假设规则是否正确，每次对根据实例数据库产生的一组假设规则进行测试，产生一组正确的规则集，最后将该规则集中前件数目最少的那条规则放入规则库。
　　该类算法最大的特点在于它是一个通用型的规则抽取算法，不受特定学习模型的限制，可以从各种神经网络以及符号学习系统中抽取出所需的规则。在输入属性分量较多的情况下其算法的计算复杂度比其它算法要低。该类算法的主要缺陷在于假设规则的产生方式限制其仅适用于处理离散属性，而不适合连续属性的处理。如果要处理连续属性，则需要在规则抽取之前将所有连续属性进行离散化处理。
　　(3) 基于VI分析(Validity-Interval Analysis)的规则抽取算法[11]
　　Thrun提出了一种基于有效区间分析(Validity-Interval Analysis)的规则抽取算法。该算法将整个神经网络结构看成一个黑盒，只考虑输入对输出产生的影响，而不考虑隐含层结点。因此算法抽取的规则如公式(1)。

THEN 此组xi表示的概念为真　　(1)
其中xi表示某个输入或输出属性，表示对该属性的取值空间的一个划分。算法将从训练实例中产生的最特殊的规则作为种子规则，用VI分析技术对该规则进行测试，根据结果扩大对取值空间的划分直至不能扩大为止。不断循环产生新的种子规则并用上述方法改进规则，直至到达某个结束标准。
　　该算法不受神经网络结构的限制，产生的规则直接从输入分量映射到输出分量，不受中间神经元多少的影响，适用于各种多层神经网络，且对输入属性的取值没有限制。抽取出来的规则精度比较高，但是以区间的形式表示规则前件使得规则的可理解性很低。而且该算法的计算开销十分昂贵，在实际应用中是不可行的，因此该算法的主要用途是进行规则测试，而不是用于规则抽取领域。
　　(4) Benitez等提出的算法
　　Benitez、Castro和Requena[11]提出标准的三层前馈神经网络学习系统和基于模糊规则的学习系统是等价系统。文章证明对于任意使用布尔函数作为中间层神经元激活函数的三层前馈神经网络均存在一个对应的FAS(fuzzy Additive System)。神经网络中的每对(隐含神经元j，输出神经元k)都可以直接对应以下的一条模糊规则如公式(2)。
　　(2)
其中A代表R上的模糊集，其隶属度函数为神经网络的激活函数，xi代表第i个输入神经元的输入值，wij代表第i个输入神经元和第j个中间神经元之间的权值，τj为第j个中间神经元的偏移值，yk代表第k个输出神经元的输出值，βjk代表第j个中间神经元和第k个输出神经元之间的权值。根据三层前馈神经网络的基本算法可以简单证明上式的正确性。虽然用该方法可以简单地抽取规则，但规则的可理解性很差，在实际应用中用途不大。
4 进一步的研究
　　作为一种获取神经网络结构中隐含的知识的手段，神经网络规则抽取算法无疑获得了较大的成功，它送给人们一把打开知识丰富的诤的钥匙。但是现有的神经网络规则抽取算法在应用中仍有很多不足之处。对这些不足之处的改进已经成为新的研究目标。进一步的研究主要集中在以下方面：
　　(1) 连续属性的处理
　　大部分规则抽取算法受限于神经网络结构和应用范围，主要原因是只能处理离散的输入属性，而对连续输入属性无能为力，产生这个问题的原因在于简单的规则本身对连续属性的表达能力不够，可以考虑适当地引入其它的规则表达形式，比如使用比较简单的模糊规则来处理连续属性，采用回归树(Regression trees)作为抽取的规则的表达形式[4]。也可以采用比较好的聚类算法，在规则抽取过程中根据需要对连续属性离散化，而不在训练神经网络时进行离散化工作。
　　(2) 算法效率的提高
　　规则抽取算法的计算复杂度可能是该技术进一步扩宽应用的一个很重要的限制因素。因此如何提高算法的效率，降低计算复杂度也是今后工作的重点。其解决方法可以从两方面进行考虑：一是可以采用各种剪枝算法降低神经网络结构的复杂性；二是可以通过减少搜索学习空间，采用各种启发式搜索策略来降低计算复杂度。
　　(3) 规则表示形式的研究
　　现有的算法在抽取出来的规则数目以及规则表示的复杂性方面也有待提高。可以考虑其它形式的规则表示方式，比如采用带优先度的规则表示方式，降低规则的数目和规则表示的复杂性；另外，也可以考虑对抽取的规则集采用剪枝算法[1]，提高规则的可理解性和规则的泛化能力。
　　(4) 从神经网络集成中抽取规则
　　目前，神经网络集成(Ensemble)[13]逐渐成为神经网络领域的一个研究热点。该技术来源于机器学习界目前极热门的Boosting方法[14]，虽然神经网络集成能提高预测精度，但其可理解性比单一神经网络更低，因此研究如何从神经网络集成中抽取易于理解的规则或规则集日益成为规则抽取领域的一个重要研究方向。
5 小结
　　缺乏对隐含知识的解释能力一直是阻碍神经网络技术被广泛应用的重要因素之一。从神经网络中抽取规则的研究工作，除了可以用易于理解的方式解释神经网络学习到的知识外，对于知识发现的研究，专家系统中的知识获取工作的研究，以及研究如何融合神经网络技术和符号学习技术的研究都很有参考价值。
　　现有的各种神经网络规则抽取算法都还存在着不同的缺陷，例如算法的计算复杂性较高，规则的可理解性较差等。我们相信，随着规则抽取研究的不断深入将会使神经网络技术得到更为广泛的应用。
本课题获得国家自然科学基金资助
孙晨（南京大学计算机软件新技术国家重点实验室 南京 210093）
周志华（南京大学计算机软件新技术国家重点实验室 南京 210093）
陈兆乾（南京大学计算机软件新技术国家重点实验室 南京 210093）
参考文献
1，Craven M, Shavlik J. Learning Symbolic Rules Using Artificial Neural Networks. In: Proceedings of the Tenth International Conference on Machine Learning, Amherst, MA, Morgan Kaufmann, 1993, 73～80
2，Alan B.Tickle,Robert Andrews, The Truth Will Come to Light:Directions and Challenges in Extracting the Knowledge Embedded Within Trained Artificial Neural Networks. IEEE TRANSACTIONS ON NEURAL NETWORKS,Vol 9,NO 6 NOVEMBER 1998 1057～1068
3，Stephen I.Gallant, Connectionist Expert Systems Communication of the ACM. Vol 31,NO 2 1988 152～169
4，Towell G.G Shablik,J.W,(1993), Extracting Refined Rules From Knowledge-Based Neural Networks Machine Learning Vol.13 ,1, 1993
5，Fu,L.M (1991) Rule Learning by Searching on Adapted Nets. In Porceeding of the Ninth National Conference on Artificial Intelligence,pp 590～595, Anaheim,CA:AAAI Press
6，Setiono R. Extracting Rules from Neural Networks by Pruning and Hidden-Unit Splitting. Neural Computation, 1997, Vol.9, No.1, 205～225
7，Craven M, Shavlik J. Extracting Tree-Structured Representations of Trained Networks. In: Touretzky D, Mozer M, Hasselmo M, eds., Advances in Neural Information Processing Systems (Volume 8), The MIT Press, Cambridge, MA, 1996, 24～30
8，Saito K, Nakano R. Rule Extraction from Facts and Neural Networks. In: Proceedings of the International Neural Network Conference (INNC-90), Paris, France, Kluwer Academic Publishers, Netherlands, 1990, 379～382
9，Craven M, Shavlik J. Using Sampling and Queries to Extract Rules from Trained Neural Networks. In: Proceedings of the Eleventh International Conference on Machine Learning, New Brunswick, NJ, Morgan Kaufmann, 1994, 37～45
10，Thrun S Extracting Rules from Artificial Neural Networks with Distributed Representations. In: Tesauro G, Touretzky D, Leen T, eds., Advances in Neural Information Processing Systems (Volume 7), The MIT Press, Cambridge, MA, 1995
11，Benitez J, Castro J, Requena I. Are Artificial Neural Networks Black Boxes? IEEE Transactions on Neural Networks, 1997, Vol.8, No.5,.1156～1164.
12，张朝辉等. 利用神经网络发现分类规则. 计算机学报, 1999, Vol. 22, No.1, 108～112
13，Igelnik B,Pao Y, The Ensemble Approach to Neural-Network Learning and Generalization IEEE Transactions on Neural Networks ,1999,10(1):19～30
14，Schapire R. The Strength of Weak Learnability. Machine Learning, 1990, 5: 197～227
收稿日期：1999年9月6日
