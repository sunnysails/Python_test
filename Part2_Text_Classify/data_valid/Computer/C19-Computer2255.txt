计算机应用研究
APPLICATION RESEARCH OF COMPUTERS
2000　Vol.17　No.5　P.6-8



基于字统计语言模型的汉语语音识别研究
吴应良　韦岗　李海洲
摘  要　隐马尔可夫模型(HMM)由于较好地描述了语音的特性，在语音识别的研究中占主导地位，基于HMM的识别算法也因取得了较好的识别效果而得到广泛应用。但其仅仅依靠语音信号的声学模型来进行识别处理，因此存在着不能利用语言的非声学知识进行识别的固有缺陷。该文提出的新方法将基于N元文法(N-gram)的统计语言模型应用于汉语语音识别，推导了模型参数的估值公式，并给出了模型的训练和识别算法。初步实验表明：引入统计语言模型有利于降低识别难度和改善语音识别性能。
关键词　统计语言模型  N-gram文法   汉语语音识别
1  引言
　　在语音识别研究领域中，由于隐马尔可夫模型(HMM)较好地描述了语音现象的特性，基于HMM的语音识别算法也取得了比较好的识别效果而得到广泛的应用，因此HMM成为当前最为流行的语音识别模型，在语音识别研究领域中占据主导地位。另一方面，虽然HMM取得了很大的成功，将语音识别的研究向前推进了一大步。但在大字表(Large vocabulary)等语音识别系统的研究过程中，人们发现，现有方法和系统的性能远不能令人满意。这说明仅仅依靠基于语音信号处理的对语音信号建立声学模型(Acoustic model)的方法[1～4]，对语音信号的描述和处理能力还不够精确和完善，还有许多能用于语音识别的语言信息还没有加以利用。对人类认知机制的研究表明，人类自身在进行自然语音识别时，不仅仅是用人耳对语声进行捕捉和辨认，同时还利用了许多非声学的信息，诸如句法、语义、语境等方面的知识来进一步对话语做出识别和理解。因此，对这些高层次的非声学知识建立适当的模型，结合到语音识别系统中，将有利于提高现有语音识别系统的识别能力和性能。
　　目前，正在研究的语言模型有两种：统计性语言模型和确定性语言模型。而将基于语料库和概率统计方法的统计语言模型(Statistical Language Modeling)应用于语音识别，成为语音识别特别是大字表和连续语音识别重要研究方向之一[５～７]。
　　本文论述了在语音识别系统中引入语言模型的原理和实现技术，讨论了两个重要的概念-语言熵(Entropy of Language)和语言模型的复杂度(Perplexity of Language)。在此基础上，本文提出了基于字的N-gram模型(Character-based N-gram Model)的汉语语音识别方法。该方法既具有单字Bigram占空间少的优点，又可充分利用基于词(Word-based)的Bigram模型及算法的优点。实验表明，该方法能有效降低语音识别的难度，容易实现且具有较高的识别率。
2  语音识别数学模型和汉语N-gram模型
2.1  语音识别数学模型
　　设A是语音的声学信号，对应的拼音串为A=(a1, a2, ..., an)。W是句子或词的序列，W=(W1, W2, ..., Wn), Wi={Wij}是拼音ai所对应的汉字(词)候选集，i=1, 2, .., n；j=1, 2, ..., mi，句子S是从候选集中得到的识别结果。语音识别的任务就是求取最佳匹配S。由最大后验概率准则有：
　　(1)
由Bayes公式有：
　　(2)
由于A在这里是不变的，故可不予考虑，因此上式可写成：
　　(3)
由(3)式可以看出，式中P(A|W)是由声学模型匹配计算得到的概率，而P(W)则与声学信号无关，即是由非声学模型即语言模型(Language Modeling)得来的。按照(2)式，我们可以给出语音识别系统的一般概念模型框图，如图1所示。

图1语音识别系统一般性模型框图
在没有语言模型的情况下，对于所有的句子W，其P(W)相同，这时有：
　　(4)
这就是仅用声学模型进行语音信号匹配时的语音识别。下面仅讨论语音识别中的统计语言模型。
　　为了定量刻划语言模型的特征，下面应用信息论讨论自然语言和语言模型的有关概念和定义：自然语言和语言模型的熵和复杂度。
　　根据信息论的观点，我们可以把语言看成为信息源，其输出的句子可以看作是词串，由n个词组成，即W=(W1, W2, ..., Wn)，又设V为词汇表(Vocabulary)或语料库(Corpus)中词的个数为V。
　　我们给出如下定义：
　　1)语言的熵(LP)
　　取每个词的熵为自然语言的熵，记为LP，即：
　　(5)
　　2)语言的复杂度(PP)
　　我们定义自然语言的复杂度为：
　　(6)
　　下面讨论和分析语言模型的熵和复杂度及其性质。
　　3)语言模型的熵(LMP)
　　对于有意义的句子，句子中词与词之间是相关的，因此考虑到这一相关性，由(5)式可以得出：
　　(7)
　　式(7)中各条件熵从概率上反映了词与词之间的相关信息或搭配关系，我们将它定义为语言模型的熵。在不同的约束条件下，可以得到不同的简化的语言模型，因而由(7)式可得到不同简化结果。
　　①只有声学模型的、孤立词、大字表语音识别系统
这时，V个词被看成等概率出现，句子这词与词之间互不相关，因此由(7)式有：
　　(8)
　　②孤立词大字表语音识别系统
　　这时，句子中词与词互不相关，但词非等概率出现，因此由(7)式有：
　　(9)
　　4)语言模型的复杂度(记为LPP)
　　类似于自然语言复杂度的定义，我们定义语言模型的复杂度为：
　　  (10)
　　对应于上述第3点的两种情况下相应简化语言模型的复杂度分别是：
　　  (11)
　　  (12)
　　由概率论和上述有关定义，可以推导出上述熵和复杂度的性质如下：
　　性质1：
　　  (13)
　　性质2：
　　  (14)
2.2  汉语N-gram模型
　　公式(7)中的条件熵反映了语言中词与词之间的搭配关系，需要我们通过建立语言模型来求取。但自然语言的丰富性使语言的建模存在很大困难。在实际处理中，为了得到切实可行的语言模型，我们必须对理论上的语言模型做出某些简化性的假设，这就是语言模型实现技术的研究任务。
　　近几年来，基于大规模语料统计的Markov语言模型方法在语音识别方面取得了较大的成功[５～７]，其克服了传统的基于规则的语音识别方法的缺点，具有可以处理大规模真实文本、处理简单、速度快等优点。N-gram模型就是统计语言模型的实现技术模型之一。下面讨论汉语的N-gram模型。
　　从(3)式和图1可以看出，语言模型的实现归结为估计概率P(W)。根据概率公式，概率P(W)可以写成：
　　  (15)
　　式(15)表明：第i个词Wi出现的概率与整个上下文W1，W2，...,Wi-1即历史有关。显然，在实际应用中计算(15)式中的条件概率是不可能的，因为当n很大时，其是一个NP问题。因此在实际应用中，我们假设自然语言是一个Markov链，即一句话中的某个字只与其前面N-1个最近的字有关。这样式(15)可写成：
　　  (16)
　　式(16)可从大规模语料统计中得到。这种利用前N-1个汉字来推测当前这个汉字的Markov模型称为N-gram(N元文法)模型。当N=1，2，3时，分别称为Unigram模型、Bigram模型和Trigram模型。如果统计的单元是单字，则称为基于字N-gram模型。在实际应用中，N较小时(N≤3)，N-gram模型才比较切实可行。
　　式(16)式可以通过Viterbi算法求出。
3  训练和识别算法
　　下面我们讨论基于单字统计的Bigram语言模型的训练和识别算法。
3.1  训练算法
　　N-gram模型当N=1时，成为Unigram模型，即词与词之间互相独立，完全没有上下文信息，反映的只是词频统计特性。这时有：
　　  (17)
　　当N=2时，即为Bigram模型，这时：
　　  (18)
　　通过对大量训练文本里词对(Word pairs)(Wi-1, Wi)出现的次数C(Wi-1,Wi)来估计统计概率：
　　  (19)
由此可进一步对P(W)进行估计。
　　设词汇表为V={W1,W2,...,WV}，个数为V。对于Unigram模型和Bigram模型的模型训练算法如下：
　　(1)初始化;
　　(2)for i=1 to V;
　　(3)从训练语料里取Wi、Wi-1∈V,由式(17)和式(19)分别计算词频度P(Wi)和词二元同现频度P(Wi|Wi-1)；
　　(4)如果P(Wi)=0或P(Wi|Wi-1)=0,则取P(Wi)=ε或P(Wi|Wi-1)=ε(ε为一设定的最小概率，为很小的非零常数);
　　(5)i=i+1; if (i&gt;V)算法结束;
　　(6)否则goto(2);
　　(7)将所有的概率进行规一化计算。
3.2  识别算法
　　对于给定的词系列W=(W1, W2, ..., Wn)，用Viterbi算法进行最优路径搜索，求得每个模型的最佳匹配，即由
　　(20)
求得似然值最大的模型就是识别结果。
4  实验结果和讨论
4.1  实验数据
　　实验针对大字表汉语语音识别。实验中使用的语料大约100万词次(人民日报文章)，语料分成两部分,分别记为CORPUS1和CORPUS2。实验分两步进行：
　　首先，我们用第一部分进行单字二元同现统计，选取CORPUS1中出现频度最高的1,000个词作为高频词集，形成统计数据库。另外，考虑到标点符号与句子开头情况，将句中标点{“，”，“、”}和句末标点{“；”，“！”，“？”，“：”，“。”}作为特殊的高频词放在高频词库中，这样高频词库共有1,008个词。测试中还用到一个一字词库，共6,773个汉字(即国标一、二汉字库)；一个二字词库，共7,683个二字词。
　　然后，进行识别实验。我们比较了两种方法：一是没有语音模型的识别；另一种是加入语言模型Unigram和Bigram的识别。测试语料中，CORPUS1为封闭语料，CORPUS2为开放语料。实验时，从CORPUS1和CORPUS2中随机抽取若干组数据进行测试。
4.2  实验结果
　　测试结果如表1和表2所示。其中，表1是复杂度比较表。
表1 复杂度比较表

文本大小复杂度语言模型情况
无语言模型Unigram模型Bigram模型
40008PPu6773409.6166.3
29782PPb76832235.9885.0

表2 识别率测试结果

前n选识别　率语言模型无语言模型Unigram模型Bigram模型
R1R2R1R2R1R2
170.6842.6380.7557.4980.6558.64
280.7258.7387.1167.8392.5969.07
390.3164.3893.5978.2696.7980.12
696.7788.5296.7788.5397.1790.10
8100.0090.80100.0090.80100.0090.80

其中，PPu为一字词的复杂度，PPb是二字词的复杂度。结果表明：有语言模型时的复杂度比没有语言模型时大为降低，而采用高阶N-gram模型比采用低阶N-gram模型的复杂度有进一步地降低。
　　表2给出了从CORPUS2中抽取的一段文本TEST(其中含词与标点180个，在一、二字字表中具有的字词150个)进行测试的识别率结果，其中R1为一字词的识别率，R2为二字词的识别率。
　　①从实验结果可以看出，引入语言模型后，语音识别率有明显的提高，而且高阶模型比低阶模型的改进效果更明显，这与理论预测相一致。
　　②统计语言模型的特点是从大量的实际语言材料以自组织方式获取其中的语言结构信息。因此，采用不同领域类型的语料，以及保证语料具有足够的规模，是建立统计语言模型的重要基础。本文中实验用到的语料的选料领域比较狭窄，量也较少。因此不够充分，会影响到语言模型的性能。
　　③N-gram模型的一个重要问题是训练数据不足引起的数据稀疏，因此如何克服这一问题，又不至于导致系统开销的增大，是一个十分重要的课题。 
5  结论和今后的工作
　　本文提出的基于统计语言模型的语音识别方法，与基于声学模型相结合，能有效地降低语音识别的难度，并能有效地提高识别效果。在今后的工作中，我们将进一步研究如下问题：
　　(1)采用有效的概率统计技术来改进N-gram模型，以解决数据稀疏问题。
　　(2)研究采用不同统计单元(如词、类)的N-gram模型，以进一步提高识别率。
　　(3)改进和提高统计语言模型的自适应性。
本项目获国家自然科学基金部分资助
吴应良(华南理工大学工商管理学院电子与通信工程系   广州 510641)
韦岗(华南理工大学工商管理学院电子与通信工程系  广州 510641)
李海洲(华南理工大学工商管理学院电子与通信工程系  广州 510641)
参考文献
1，P. F. Brown, The Accoustic-Modeling Problem in Automatic Speech Recongition, PH.D. Thesis CMU, May 1987
2，L. R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. Proc.IEEE, Feb.1989, 77(2): 257～286
3，梅  勇, 王群生, 徐秉铮. 基于模糊神经网络的声母识别. 电子科学学刊，1998，20(2)，188～193
4，杨浩荣，王作英，陆大. 语音识别HMM中引入帧间相关信息的一种参数化模型. 电子学报，1998，26(10): 50～54
5，吴应良，李海洲. 基于N-gram模型的汉语分词研究. 广州：中国电子学会电路与系统学会第十五届年会论文集，第323～327页，1999年11月
6，Yamamoto M.: A Re-estimation Method for Stochastic Language Modeling from Ambiguous Observations, in Proceeding of WVLC-96,pp.155～167,1996
7，Rosenfeld R. The CMU Statistical Language Modeling Toolkit and Its Use in the 1994 ARPA CSR Evaluation, In the Proceedings of ARPA Spoken Language Systems Technology Workshop,pp.47～50, 1995
8，L. R. Bahl, P.F. Brown, P.Desouza, R.L.Mercer. A Tree-Based Statistic Language Model for Natural Language Speech Recognition, K.F.Lee, Alex Wabel, 1990
9，关  毅，张  凯，付国宏. 基于高阶N-gram的单词聚类和规则获取算法的研究. 计算机应用研究，1999,(5):15～17
收稿日期：1999-12-18
