信息与控制
INFORMATION AND CONTROL
1999年 第28卷 第6期 Vol.28 No.6 1999



自学习神经元及自学习BP网络
赖晓平 周鸿兴
　　摘　要：本文针对现有人工神经元及BP网络的缺点，从实现角度提出一种新型神经元及新型BP网络――自学习神经元及自学习BP网络．自学习神经元的突出特点之一是它的内部有正向通道、反向通道及学习器，因而能够独立完成信息的正向传播、误差的反向传播及神经元参数的修正．由自学习神经元组成的自学习BP网络可以真正做到正向传播信息、反向传播误差及学习的并行化．本文还考虑了自学习BP网络的学习问题，提出一种新的学习策略．我们的仿真结果表明这种学习策略有很好的学习效果．
　　关键词：自学习神经元，自学习BP网络，学习策略，面向神经元
　　中图分类号：TP13　　　　　　文献标识码：A
SELF-EARNING NEURONS AND SELF-LEARNING BP NETWORKS
LAI Xiao-ping1 ZHOU Hong-xing2
(1. Department of Control Engineering, Shandong University at Weihai, Weihai 264209; 2. Department of Mathenatics, Shandong University, Jinan 250100)
Abstract: A new king of neurons and new kind if BP networks, self-learning neurons and self-learning BP networks, are presented in this paper with a wiew to their implementation. A self-learning BP network is composed of self-learning neurons. One of the prominent characteristics of the self-learning neuron is thatit has a forward-channel, a backward-channel and a leamer, This makes each neuron in a self-learning BP network accomplish the forward propagation of message, backward propagation of errors, and modification of its parameters independently, and hence makes the network implement parallel computing easity. A new training policy for BP networks is also presented in this paper. Simulation results demonstrate the effectiveness of the policy.
　　Key words: self-learning neurons, self-learning BP networks, training policy, neuron-oriented algorithm
1　引言
　　多层前向网络是研究和应用得最广泛也是最成功的人工神经元网络模型之一．多层前向网络是一种映射型网络，理论上，隐层采用Sigmoid激活函数的三层前向网络能以任意精度逼近任一连续非线性函数．神经元网络可以根据与环境的相互作用对自身进行调节即学习．一个BP网络即是一个多层前向网络加上误差反向传播学习算法，因此一个BP网络应有三项基本功能：（1）信息由输入单元传到隐单元，最后传到输出单元的信息正向传播；（2）实际输出与期望输出之间的误差由输出单元传到隐单元，最后传到输入单元的误差反向传播；（3）利用正向传播的信息和反向传播的误差对网络权系数进行修正的学习过程．目前，多层前向网络的权系数学习算法大都采用BP算法及基于BP算法的改进算法，如带动量项的BP算法，同时修正权系数及激活函数的BP算法[1,2,3,4]及自适应调整学习步长的BP算法[2,3]等．在这些算法中，同时修正权系数及激活函数的BP算法及自适应调整学习步长的BP算法具有较好的收敛特性．BP网络在非线性系统的辨识、预测及控制等问题中有很好的应用．
　　但是，当我们仔细分析现有BP网络结构、权系数学习算法及其实现时就会发现以下的局限性：（1）现有的BP网络结构只能完成信息的正向传播任务，误差的反向传播及权系数的学习都是人为地附加在网络上的；（2）从表面上看，现有的BP网络具有并行结构，但实际上由于学习算法都是面向整个网络的，因而不能做到计算的并行化；（3）现有的BP网络的神经元之间的连接是带权连接，从实现角度考虑，当由神经元组成网络时，不太方便，从概念上讲，当网络由单层（或单个）神经元组成时，不再有神经元间的连接，因而不再有连接权系数，这是矛盾的；（4）当网络中某个神经元的权系数受到冲击而出现偏差需重新学习时，由于算法是面向网络的，不得不对整个网络的权系数都重新学习；（5）现有的BP网络学习算法对所有神经元的连接权系数都采用同一算法进行学习，缺乏灵活性．
　　本文针对现有BP网络的这些问题，提出一种新型的神经元及新型的BP网络．这种新型的神经元内有传播信息的正向通道、传播误差的反向通道及利用正向传播的信息及反向传播的误差进行学习的学习器．信号权系数、偏量权系数及激活函数的陡度都认为是神经元的内部参数，学习器的任务就是调整这些参数．由这种新型的神经元连接在一起构成一种新型的BP网络，它将有以下特点：（1）能够完整地实现BP网络的三项基本功能，即正向传播信息、反向传播误差及参数的学习；（2）信息的正向传播、误差的反向传播及参数的学习都是并行的，从而真正做到计算的并行化；（3）神经元之间的连接是无权的，实现非常方便；（4）对神经元参数的学习采用面向神经元的算法，从而当某个神经元受到冲击，参数出现偏差时，可以单独对这个神经元进行学习，加快学习的速度；（5）不同的神经元可以采用不同的学习算法，从而构成各种学习策略，极大地提高了BP网络学习的灵活性．
　　本文还将研究BP网络的学习问题，提出一种新的学习策略，这种学习策略的收敛特性比通常的同时修正权系数及激活函数的BP算法[1,2,3,4]及自适应调整学习步长的BP算法[3,4] 都好得多．本文给出的仿真例子也说明了这一点．
2　自学习神经元
　　定义1 称一个神经元为自学习神经元，如果它具有正向、反向及学习等三种内部状态，且当它处于这三种状态时能分别实现正向传播信息、反向传播误差及调节自身参数等三项基本功能．
　　(1)信息的正向传播 当自学习神经元处于正向状态时，神经元进行信息的正向传播．输入信号u=(u1, u2,…, un)τ与偏置量1一起组成输入向量φ=(1, u1, u2,…, un)τ，φ与权系数向量w=(w0, w1, w2,…, wn)τ点乘，产生净输入net=φτw，net与系数μ相乘得到总输入z=μnet，z通过激活函数f(.)作用产生神经元的活性o=f(z)，再与全一向量σ=(1,1,…,1)τ相乘得到输出向量o=(o, o,…, o)τ∈Rm．
　　权系数向量w中的w0称为偏量权系数，w1, w2,…, wn称为信号权系数．激活函数通常取Sigmoid函数：非对称的f(x)=[1+exp(-x)]-1或对称的f(x)=[1-exp(-x)][1+exp(-x)]-1，也可以取线性函数f(x)=x．系数μ称为激活函数的陡度．调整μ可以改变神经元的活性o关于净输入net的变化率，这相当于改变神经元的激活函数．


图1　正向状态

图2　反向状态
　　正向状态时自学习神经元的外部特性如图1所示，SI代表信号输入端，SO代表信号输出端．
　　(2)误差的反向传播 当自学习神经元处于反向状态时，神经元进行误差的反向传播．其外部特性如图2所示，正向状态时的SO此时为误差输入端EI，正向状态时的SI此时为误差输出端EO．输入误差e=(e1, e2,…, em)τ由EI端输入到神经元内部，与全一向量σ点乘后再与f′(z)(f′(x)为f(x)的导数)相乘产生总误差，经μw加权得到输出误差ε=(ε1,ε2,…,εn)τ=δμ(w0, w1, w2,…, wn)τ由EO端输出．
　　(3)参数的学习――面向神经元的学习算法 自学习神经元的参数包括信号权系数w1,w2,…,wn，偏量权系数w0及激活函数的陡度μ，称θ=(w0,w1,w2,…,wn, μ)τ=(wτ,μ)τ为神经元的参数矢量．神经元处于学习状态时，由内部的学习器对参数矢量θ进行修正，利用正向传播的信息(如φ、net)、反向传播的误差δ及旧的参数矢量θold=(wτold,μold)τ通过某一学习算法计算新的参数矢量θnew=(wτnew,μnew)τ．这里所采用的学习算法是面向神经元的――利用神经元内存储的信息修正神经元自己的参数，学习算法本身的设计参数可由外界设定．面向神经元的学习算法采用统一的形式
θnew=θold +Δθnew　　　　　　　　　(1)
其中的Δθnew =(Δwτnew,Δμnew)τ，不同的算法有不同的计算公式．下表给出了一些面向神经元的学习算法．
表1 面向神经元的学习算法

算法ΔwnewΔμnew设计参数
算法1ηδμoldφ0η＞0
算法2αΔwold+ηδμoldφ00＜α＜1, η＞0
算法30γΔμold+kδnet0＜γ＜1, k＞0
算法4αΔwold+ηδμoldφγΔμold +kδnet0＜α＜1, η＞0, 0＜γ＜1, k＞0

　　为了实现自学习神经元的三项功能，神经元内部应有传播信息的前向通道、传播误差的后向通道及修正神经元参数的学习器．图3是自学习神经元的一种实现．K1和K2都连到F位置时构成前向通道，自学习神经元处于前向状态．K1和K2都连到B位置时构成后向通道，自学习神经元处于后向状态．自学习神经元在三个状态间交替变换，在正向状态和反向状态之后，学习器利用正向状态时传播的信息及反向状态时传播的误差调整神经元自身的参数时，自学习神经元处于学习状态．


图3 自学习神经元
3　自学习BP网络
3．1　自学习BP网络及与普通BP网络的区别
　　定义2 由自学习神经元采用无权连接构成如图4所示有一个输入层、若干个隐层、一个输出层另加一个比较层的神经元网络，称为自学习BP网络，用记号表示为N(n1, n2, …, nM-1, nM)．这里M≥3，ni表示第i层神经元个数，i=1表示输入层，i=2, …,M-1表示隐层，i=M表示输出层．


图4 自学习BP网络
　　自学习BP网络的比较层神经元有正向状态和反向状态．正向状态时将输出神经元的输出和期望输出y进行比较．反向状态时将正向状态时的比较误差（y-）加权后形成加权误差并将其反向传输．
　　自学习BP网络输出层神经元的激活函数一般取为线性函数：f(x)=x ．输入层神经元的SI/EO端只正向输入信息，不反向传播误差．每个输入神经元只接受一维输入信号，信号权系数等于1，偏量权系数等于0，激活函数的陡度可变，因而输入层神经元的权系数向量可表示为：w1j=(0,1)τ (j=1, 2, …, n1)，其中上标1表示第一层即输入层，下标j表示第j个神经元．输入层神经元只有激活函数的陡度需要学习．
　　虽然从表面上看，自学习BP网络和普通的BP网络很类似，但它们之间是有区别的．与普通BP网络相比，自学习BP网络有以下五个优点．
　　（1）由于自学习神经元内的权系数向量可以对神经元的输入信号及反传误差加权，从而自学习BP网络神经元之间的连接是无权的，不论对软件实现还是对硬件实现，这都是非常方便的．
　　（2）自学习BP网络本身具有正向传播信息、反向传播误差及对神经元参数进行学习的能力．当每个神经元都处于正向状态时，网络可正向传播信息，当每个神经元都处于反向状态时，网络可反向传播误差，当每个神经元都处于学习状态时，网络可对网络参数进行修正．
　　（3）自学习BP网络的各个神经元除它们之间的连接关系外相对独立，从而可以实现信息正向传播、误差反向传播及参数调整的并行化．
　　（4）自学习BP网络中的每个神经元都有自己的学习器，采用的学习算法是面向神经元的，因而各个神经元可以采取不同的学习算法．
　　（5）各个神经元的学习器是相互独立的，因此当某个神经元受到冲击需要重新学习时，可以只修正这一神经元的参数，其它神经元的参数保持不变．这种学习方式比修改所有神经元参数的学习方式有更快的收敛速度．
3．2 自学习BP网络的工作原理
　　由于每个自学习神经元都有正向传播信息、反向传播误差及学习的功能，且神经元之间的连接又是无权的，因而BP网络的大部分工作都分散到了各个神经元．由自学习神经元构成的自学习BP网络也有三种状态：正向状态、反向状态及学习状态．当网络中所有神经元都处于正向状态时，网络为正向状态．所有神经元都处于反向状态时，网络为反向状态．所有神经元都处于学习状态时，网络为学习状态．网络状态间的转换由网络本身负责．另外，神经元的学习算法有些由外部设定的设计参数需要由网络来给定．
　　一旦将自学习神经元连成自学习BP网络，神经元之间的信号关系就是确定的．设网络的输入为x=(x1, x2,…, xn1)τ，期望输出为y=(y1, y2,…, ynM)τ，实际输出为y=(1, 2,…, nM)τ．则网络在正向传播信息的过程中，各层神经元的输入输出信号之间有如下关系：
ulj=xj (j=1,2,…,n1)　　　　　　(2)
　　　　　(3)
j=oMj (j=1,2,…,nM)　　　　　　　　(4)
其中uij表示第i层第j个神经元的输入信号，i=1时是标量，i>1时是ni-1维向量，oij表示第i层第j个神经元的输出信号．网络在反向传播误差的过程中各层神经元的输入输出误差之间有如下关系：
eMj=λj(yj-j) (j=1,2,…,nM)　　　　　　　　(5)
　　　　　　　　　(6)
其中eij表示第i层第j个神经元的输入误差，i=M时是标量，i<M时是ni+1维向量，eijk是其第k个分量．λj>0为比较层第j个神经元对比较误差（yj-j）的加权因子．εikj表示第i层第k个神经元的输出误差εik的第j个分量．
　　定理1 设自学习BP网络为N(n1, n2, … , nM-1, nM )，定义网络对样本(x, y)的匹配误差为
　　　　　　　(7)
其中是网络在输入x下的实际输出，Λ=diag(λ1,λ2,…,λnM), λj >0（j=1,2,…,nM）等于网络比较层第j个神经元的加权因子，则：
　　　　　　　(8)
　　　　　(9)
　　　　　　(10)
　　证明 先证（8）式．当i=M时，

当i=M-1, M-2, …, 1时，

（8）式得证．
　　由（8）式，立即有：

定理证毕．
　　定理1表明，若样本匹配误差如（7）式定义，则自学习BP网络中神经元的总误差δ就是普通BP网络中的反向传播误差δ，从而面向神经元的学习算法1―学习算法4 对参数的更新都是沿匹配误差对参数的负梯度方向即最速下降方向进行的．
4　自学习BP网络的学习策略
　　由于自学习BP网络中的神经元内部有参数学习器，其中的学习算法是面向神经元的，因而对网络中的各个神经元可以采取不同的学习算法，从而构成各种学习策略，这是自学习BP网络和普通BP网络相比的一个突出优点．面向神经元的学习算法可以是算法1-算法4中的任何一种，也可以是其它类型的算法如最小二乘算法等．
　　学习策略1：输入层神经元参数固定，隐层神经元及输出层神经元均采用算法1调整参数，其中的设计参数η=常数．
　　学习策略2：输入层神经元参数固定，隐层神经元及输出层神经元均采用算法2调整参数，其中的设计参数α及η均取常数．
　　学习策略3：输入层神经元参数固定，隐层神经元及输出层神经元均采用算法4调整参数，其中的设计参数α、η、γ及k分别取不同的常数值．
　　由定理1 的（9）式及（10）式我们知道，若输入层神经元为线性神经元，则学习策略1相当于普通多层前向网络的BP算法，学习策略2 相当于普通多层前向网络带动量项的BP算法，学习策略3相当于同时修正权系数和激活函数的BP算法[1,2,3,4]．
　　学习策略4:自适应调整步长的BP学习策略：输入层神经元参数固定，隐层神经元及输出层神经元均采用算法1调整参数，其中学习步长η是随学习样本(x(t),y(t))变化的，时变的学习步长η(t)由下式给出：
　　　　　　　　(11)
　　　　　　　　　　(12)
其中P表示网络所有隐层神经元及输出层神经元的参数组成的矢量，即
　　　　　　(13)
J′P(t, P)表示匹配误差J(t, P)=0.5(y(t)-(t))τΛ(y(t)-(t))关于P的梯度，从而
　　　　　　(14)
0<β<1是可调参数，P0是P的初始值，J(0)是匹配误差的初始值．
　　定理2 样本集为{(x(t), y(t)), t =1,2, …}．若自学习BP网络N(n1, n2, … , nM-1, nM)采用学习策略4更新其神经元的参数，则有
　　　　　　　　　　　(15)
证明 

由学习策略4 及定理1有

从而有

再由（12）式及0<β<1知结论成立．证毕．
　　学习策略3让陡度参数参与和权系数类似的修正，从而可避免有些神经元的输出饱和，加速目标函数的收敛．学习策略4适应学习样本(x(t),y(t))的变化，参数更新的目的要使新参数下网络与样本(x(t),y(t))的匹配误差小于旧参数下网络与样本(x(t-1),y(t-1))的匹配误差（定理2）．这两种学习策略比学习策略1 及学习策略2 具有快得多的收敛速度，但也有各自的局限性．学习策略4 起步时学习速率很大，但当误差平方和指标小到一定数值以后，学习速率变小，以至指标很难收敛到很小的值．学习策略3 在误差平方和指标小到一定值以后收敛很快，但学习的起步阶段较慢，且对权系数的初始值比较敏感．这些可以从我们的仿真结果看出．本文将把这两种学习策略结合起来，避开各自的短处，形成一种新的学习策略――两阶段学习策略．下面先对学习策略3 进行一点小的修改，得到下面的学习策略5．
　　学习策略5:激活函数陡度可调的BP学习策略：输入层神经元采用算法3调整参数，隐层神经元及输出层神经元均采用算法4调整参数，算法3及算法4中的设计参数α、η、γ及k均取常数值．
　　输入层神经元的作用是将网络的输入信号变换到一个合适的范围后再传输到隐层．神经元的活性函数若为对称的Sigmoid函数，则可把输入信号变换到(-1,1)，若为线性函数，则可通过调整其陡度将输入信号变换到所需要的范围．学习策略5允许输入层神经元的激活函数陡度和隐层神经元及输出层神经元的激活函数陡度一起参与参数的修正，这将进一步提高学习过程的收敛速度．
　　学习策略6:两阶段学习策略：给定样本集Ω={(x(k), y(k)), k =1,2,…,N}，网络的整个学习过程分两个阶段，起步时用学习策略4进行学习，当网络对样本集Ω的匹配误差
　　　　　　　　　　　(16)
小于某一数值Jc>0后，改用学习策略5进行学习，直到JΩ小于某一给定值J0为止．
　　可以看到，这里给出的两阶段学习策略集中了自适应调整步长的BP学习策略和激活函数陡度可调的BP学习策略 的优点，避开了它们的短处，因而将有比这两种学习策略更好的收敛特性．现设样本集为Ω= {(x(k), y(k)), k =1,2,…,N}，自学习BP网络为N(n1, n2, … , nM-1, nM)，则采用两阶段学习策略对网络进行学习的具体过程如下：
　　(1) 初始化网络：设定各神经元的活性函数的类型，给各神经元的参数置初值．
　　(2) 给定NP（最大学习周期）、J0及Jc．令np = 1．
　　(3) 对k=1,2,…, N：
　　a. 令t=np*N+k, x(t)=x(k), y(t)=y(k)；
　　b. 进行网络的正向计算；
　　c. 进行网络的反向计算；
　　d. 用自适应调整步长的BP学习策略 更新神经元参数．
　　(4) 计算JΩ．若JΩ<Jc，则转到(5)；否则，np←(np+1)，转到(3)．
　　(5) 对k= 1,2,…, N： a. 令t=np * N+k, x(t)=x(k), y(t)=y(k)；
　　b. 进行网络的正向计算；
　　c. 进行网络的反向计算；
　　d. 用激活函数陡度可调的BP学习策略 更新神经元参数．
　　(6) 计算JΩ，np←(np+1)．若np>NP或JΩ<J0，转到(7)；否则转到(5)．
　　(7) 结束．
5　实例仿真
　　自适应调整步长的BP学习策略相当于[2]中给出的自适应调整学习步长的BP算法，激活函数陡度可调的BP学习策略则是对[1,2,3,4]中给出的同时修正权系数和激活函数的BP算法稍加修改而来，在这一节，我们分别称它们为现有学习策略A及现有学习策略B．
　　考虑一个非线性系统的辨识问题．设非线性系统为
y(k+1)=y(k)/(1+y2(k))+u2(k), y(0)=0
其中u(k)是均匀分布于[-2，2]的随机输入信号．为了仿真，产生400组数据{(u(k),y(k))τ, y(k+1)}作为学习样本集．采用结构为N( 2, 5, 1, 1 )的自学习BP网络进行建模．
　　网络的输入层神经元及输出层神经元均设为线性神经元，隐层神经元设为对称Sigmoid型．各神经元激活函数陡度的初值均取1.0，权系数的初始值则取均匀分布于(-0.5, 0.5)的随机数．在同一组权系数初始值下,分别用现有学习策略A、现有学习策略B及两阶段学习策略对样本集进行学习．现有学习策略A中β=0.8；现有学习策略B中所有神经元均取α=0.9, η=0.005, γ=0.5, k=0.005；两阶段学习策略中取Jc=10.0．通过几十次的仿真，结果表明：现有学习策略A 起步时学习速率很大，且对权系数的初始值不很敏感，但当JΩ小到一定数值以后，学习速率变小，JΩ很难收敛到很小的值；现有学习策略B起步阶段收敛较慢，且对权系数的初始值比较敏感，有时甚至不收敛，但是JΩ一旦小到一定数值，以后就收敛很快；两阶段学习策略在整个学习阶段都有很快的收敛速度，对权系数的初始值不太敏感．两阶段学习策略的收敛速度比现有学习策略A及现有学习策略B的收敛速度大四倍以上．图5给出了一次典型的仿真结果，图中纵坐标JΩ(np) 是对数刻度的，横坐标np则主刻度（10n）为对数刻度，次刻度为均匀刻度．
6　结论
　　本文提出的自学习神经元具有正向传播信息、反向传播误差及学习的功能，它是一个独立的实体．由自学习神经元可方便地构成自学习BP网络，这种BP网络有许多普通BP网络所没有的优点．本文提出的两阶段学习策略具有收敛速度快、收敛后的均方误差指标小的特点．本文的结果为BP网络的实现及BP网络的实际应用打下了坚实的基础．


图5 三种学习策略的收敛性能比较
基金项目:国家自然科学基金资助项目,项目批准号:69774002
作者简介
　　赖晓平(1965-)，男，硕士，副教授．研究领域为系统辨识，数字信号处理，神经网络理论与应用．
　　周鸿兴(1941-)，男，教授，博士生导师．研究领域为分布参数系统，DEDS理论，神经网络理论．
作者单位：赖晓平 山东大学威海分校控制工程系 威海 264209
　　　　　周鸿兴 山东大学数学与系统科学学院 济南 250100
参考文献
1　L V Fausett. Fundamentals of Neural Networks-Architectures, Algorithms and Applications, Prentice Hall, Englewood Cliffs, 1994
2　S Zhou, D Popociv, G Schulz-Ekloff. An Improved Learning Law for Back-propagation Networks. In 1993 IEEE International Conference on Neural Networks, San Francisco: IEEE press, 1993: 573～579
3　S Zhou, D Popociv, G Schulz-Ekloff. A Motivation-Followed Learning. Proceedings of 1993 International Joint Conference on Neural Networks, 1993:549～552
4　李士勇．模糊控制.神经控制和智能控制论．哈尔滨工业大学出版社，1996
收稿日期:1998-11-09
