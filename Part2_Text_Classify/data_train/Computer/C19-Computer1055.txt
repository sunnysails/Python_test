自动化学报
ACTA AUTOMATICA SINICA
1998年 第24卷 第5期  Vol.24  No.5 1998



一种回归神经网络的快速在线学习算法
韦　巍
摘　要　针对回归神经网络BP学习算法收敛慢的缺陷，提出了一种新的快速在线递推学习算法.本算法在目标函数中引入了遗忘因子，并借助于非线性系统的最大似然估计原理成功地解决了动态非线性系统回归神经网络模型权系数学习的实时性和快速性问题.仿真结果表明，该算法比传统的回归BP学习算法具有更快的收敛速度..
关键词　回归神经网络，在线学习，快速算法.
A NEW ON-LINE RECURSIVE LEARNING ALGORITHM FOR
RECURRENT NEURAL NETWORK
WEI WEI
(Department of Electrical Engineering, Zhejiang University, Hangzhou 310027)
Abstract　In this paper a new on-line recursive learning algorithm for recurrent neural network is proposed. It overcomes the disadvantage of the slow convergence of the recurrent BP algorithm. The real-time learning ability and the fast convergence of the recurrent network model of nonlinear dynamical system have been obtained by introducing the forgetting factor in the objective function and the maximum likelihood estimation principle. Simulation results show that the proposed algorithm performs better than the traditional recurrent BP algorithm.
Key words　Recurrent neural network, on-line learning, fast algorithm.
1　引言
　　传统的前向传播神经网络在动态时序信号处理、非线性动态系统控制等带有强时序行为系统的应用中存在相当大的困难.目前虽然有通过在网络中引入时滞环节来描述系统的动态性能［1］，但是能够更直接更生动地反映系统动态特性的网络应该是动态神经网络，它包含网络内部状态的反馈.回归神经网络就是利用网络的内部状态反馈来描述系统的非线性动力学行为.构成回归神经网络模型的方法有很多，但总的思想都是通过对前馈神经网络中加入一些附加的、内部的反馈通道来增加网络本身处理动态信息的能力.例如根据状态信息的反馈途径不同可构成两种不同的回归神经网络结构模型：Jordan型(如图1(a))和Elman型(如图1(b)).


(a)Jordan网络结构　　　　　　　(b)Elman网络结构
图1　回归神经网络结构模型
　　由于在回归神经网络中存在内部反馈，因此传统前馈网络的学习算法――BP法已不能直接应用于回归神经网络的学习.1987年Pineda首先提出了回归神经网络的BP学习算法［2］.遗憾的是Pineda提出的方法仍然是依据梯度法来进行的，因此不可避免地会产生局部收敛和收敛速度慢等缺陷.如何寻求一种快速的寻优算法来解决回归神经网络的学习问题已引起许多专家学者的关注.本文将非线性系统的最小二乘递推算法引到回归神经网络权阵的学习中来，利用此算法的实时性和快速性来解决神经网络的学习问题.依据回归神经网络BP学习算法的原理和方法来推导出回归神经网络的快速在线学习算法.仿真实验进一步证明了此学习算法的有效性和快速性.
2　回归神经网络模型及其BP学习算法
　　对于一个动态的目标序列tk,tk-1,tk-2,…,传统的神经元网络动态系统描述法是通过引入系统的先前输出yk-1,yk-2,…,yk-n等作为前向传播神经网络模型的输入信号，从而来描述系统的动力学特性的.最典型的此类模型结构就是Narendra K.S.等人提出的带时滞环节的MLP网络结构［1］.显然，引入过少的时滞环节即n太小则不能描述具有高阶滞后系统的动态时序特性.另一方面，引入过大的时滞环节即n太大，则需要系统更多的记忆单元来保持所有的先验信息，因此也是不可取的.回归神经网络就是针对这一缺陷而提出的.它能利用很少的记忆单元来描述任何系统的动力学特性.
　　设Y，H，U，R分别表示m维输出矢量、h维隐含单元的状态矢量、n维输入矢量和r维内部反馈状态矢量，则回归神经网络模型可以用如下方程来描述
Yk=Φ(b+β′Hk),　Hk=Ψ(c+γ′Uk+δ′Rk),　Rk=Λ(Uk,Rk-1,W)　　(1),(2),(3)
其中k表示第k时刻，Φ，Ψ，Λ分别为输出神经元、隐含神经元和回馈神经单元的矢量函数，W为s维的神经网络连接权系数阵，包括(b,β',c,γ',δ').当取Rk=Yk-1,Hk-1，0时，则此网络分别为Jordan、Elman、MLP网络结构.
　　由于引入了回归单元函数(3)，可以实现对所有先验输入数据的纪录；换言之，回归神经网络的回归变量R能够依据网络输出Y或隐含单元的状态H的信息用紧凑的形式来保留系统所有以前的信息.由式(3)递推可得
Rk=Λ(Uk,Rk-1;W)=Λ(Uk,Λ(Uk-1,Rk-2;W);W)=…=Γ(Uk;W),　　(4)
其中Uk=｛U0,U1,…,Uk｝为所有以前的输入数据信息.
　　从(4)式可以看出，回归神经网络模型是最节约的网络模型，它无需存储所有的输入信息但又能在网络中反映出系统所有的历史信号对当前系统响应的影响.正是由于回归神经网络的这一特性使得其在动态领域的应用具有很大的吸引力.
　　由(1，2)式可得
Yk=Φ(b+β′Ψ)(c+γ′Uk+δ′Rk(W))=F(Uk,Rk(W);W).　　(5)
将式(4)代入式(5)可以看出，网络的输出Yk也是系统过去所有输入Uk-1的函数.要使这样的回归网络真正能在实际中得到应用，首先需要解决此类网络的学习问题.与传统的前向传播网络一样，回归神经网络的学习指标函数仍然选用误差平方和函数，即给定的样本输出空间Zk,k=0,1,2…
　　(6)
对于动态序列Zk的网络权系数阵W的最优求解可归结为求
　　(7)
其中E［.］表示数学期望.
假设式(7)的极限存在，则由最大似然估计法可知权系数阵的更新公式为
Wk+1=Wk-ηk+1Pk+1　eTk+1.ek+1,　　(8)
其中
ek+1=Zk+1-F(Uk+1,Rk+1;Wk)，　(9)
　ek+1为ek+1对参数阵W的梯度矢量.
　　当Pk+1=I时，即为梯度法；当Pk+1=ETk+1.ETk+1时，即为牛顿法.其中Ek+1=
　　必须注意的是，由于方程(9)描述的误差矢量既是W的显函数又是Rk+1的函数即W的隐函数.因此在求梯度矢量ek+1时不但要考虑ek+1对W的显式求导而且要考虑它的隐式求导.考虑了这些因素以后，不难得出回归神经网络模型(1-3)的BP学习算法(见文献［3］).
3　回归网络的在线学习算法
　　由于考虑的是动态系统，因此在处理样本数据时应尽量加重最新的输入输出数据在网络建模中的作用.与回归神经网络的传统BP学习算法指标函数不同的是，本文提出的改进算法采用遗忘因子对Jk中的各项拟合残差平方进行加权办法来提高动态模型的实时性.即取目标函数
　　(10)
　　如果Wk是满足目标函数(10)的极小解，则有
.　　(11)
假设最大似然估计k具备良好的渐近性质，则当k比较大时k可望与真值W很接近.因此在=k点的附近e(l,)可在k点进行泰勒展开，并取一次项得
　　(12)
　　若再取得一组新的观察数据Zk+1，则可根据l=k+1时刻以前的全部观察，得出使目标函数
　　(13)
的极小解k+1.此时还应有
　　　(14)
由于k很大时，k+1≈k，故按式(12)有
　　　(15)
　　(16)
　　利用式(11)，(15)，(16)，对目标函数(10)式进行求导得
　(17)
　　将式(15)―(17)代入式(14)中可得
　　(18)
由式(18)可得
　　(19)
　　引入变量
　　　(20)
则回归神经网络的联结系数矩阵的更新公式为
k+1=k-Pk+1　e(k+1,k)e(k+1,k)，　　(21)
其中
　　(22)
　　利用矩阵反演公式可将(22)式进一步简化，从而可得出整个回归神经网络的快速在线学习算法
　　1)随机选取一个初始估计系数阵表示在第一步迭代时权系数阵按梯度方向进行更新的步长.
　　2)k←0.
　　3)取当前最新的样本数据(Uk+1,Zk+1)，按下列各式更新权系数阵
Rk+1=Λ(Uk+1,Rk;Wk),　　(23)
e(k+1,k)=Zk+1-F(Uk+1,Rk+1,k),　　(24)
k+1=ΛW(Uk+1,Rk;Wk)+k.ΛR(Uk+1,Rk;Wk),　　(25)
　e(k+1,k)=-FW(Uk+1,Rk+1;Wk)-k+1FR(Uk+1,Rk+1;Wk),　(26)
γk+1=［αIm×m+　eT(k+1,k)Pk　e(k+1,k)］-1,　　(27)
Pk+1=［αPk-Pke(k+1,k)γk+1eT(k+1,k)Pk］/α,　　(28)
k+1=k-Pk+1e(k+1,k)e(k+1,k).　　(29)
　　4)k←k+1,转3).
4　仿真研究
　　通过一个例子的仿真结果来评价本文提出的快速在线学习算法的有效性.仿真模型选用文献［4］中给出的双线性DGP模型，即
zk=0.5-0.4zk-1+0.4zk-1uk-1+uk,　　k=1,2,…,kmax,　　(30)
其中uk选择独立的随机序列N(0，2).
　　不难看出，此系统的输出依赖于系统过去的所有信息.在例中选用Elman回归神经网络结构，并取隐含单元为4.隐含神经元的激励函数为Sigmoid函数、输出单元为线性函数.在例子的仿真研究中取η=0.02,α=0.95,ρ=0.68.W0，R0，0为0―1之间的随机数.输入单元为zk-1,uk.当选用Elman回归神经网络结构时其连接系数总共有33个.利用此神经网络结构和给定的动态模型进行仿真研究，其仿真结果如图2，图3所示.


图2　两种学习算法的平方误差曲线


图3　两种学习算法的辨识结果
　　图2中的横座标表示迭代的次数，纵座标表示最近25个点的平方误差均值.其中实线表示利用本算法进行学习的平方误差收敛曲线、虚线表示用传统回归网络BP学习算法训练的平方误差收敛曲线；图3表示最后一次迭代学习结束后的最近25个点的系统响应曲线.其中横座标表示时间相对值、纵座标为系统的输出即实线表示动态模型的真实输出值、点划线表示回归网络在线学习算法得出的辨识结果、点线表示传统回归网络BP学习算法得出的辨识结果.从仿真结果可以看出，回归网络在线学习算法具备比传统回归BP算法更快的收敛性和更高的辨识精度.回归网络在线学习算法在学习过程中由于P0的正定性难以时时刻刻得以满足，因此不可避免地出现一些波动.在本算法中采取了一些措施来抑制算法可能出现的发散，如当相邻两次的目标函数上升幅度过大时权系数的更新将沿梯度方向进行.矩阵P0将重置为对角阵.从而保证了回归神经网络在线学习算法能够正常运行下去.此外，在初始学习阶段，由于Wk远离真值W，因此式(12)的近似度受到严重影响.在仿真研究中本文采取在初始的几步中选择负梯度方向进行搜索，而后转入在线递推算法的训练和学习.　
1)国家教委留学回国人员科研启动基金资助.
作者简介：韦巍　1964年出生.1986年在浙江大学获硕士学位，1994年获博士学位.现为浙江大学电机系副教授.近年来发表学术论文20余篇.目前主要研究方向为智能控制理论和应用、机器人控制.
作者单位：(浙江大学电机系　杭州　310027)
参考文献
　1　Narendra K S, Parthasarathy K.Identification and control of dynamical systems using neural networks. IEEE Trans. on N.N. 1990, 1(1):4―27
　2　Pineda F J. Generalization of back-propagation to recurrent neural networks. Physical Rev.Lett., 1987, 59:2229―2232
　3　Kuan C M, Hornik K, White H. A conversgence results for learning in recurrent neural networks. Neural Computation, 1994, 6(2):420―440
收稿日期　1996-08-12
