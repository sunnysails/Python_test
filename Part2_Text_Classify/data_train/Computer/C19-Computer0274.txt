软件学报
JOURNAL OF SOFTWARE
1999年 第19卷 第3期  Vol.19 No.3 1999



语言模型中一种改进的最大熵方法及其应用
李涓子　黄昌宁
摘　要　最大熵方法是建立统计语言模型的一种有效的方法,具有较强的知识表达能力.但是,在用现有的最大熵方法建立统计模型时存在计算量大的问题.针对这一问题,提出了一种改进的最大熵方法.该方法使用互信息的概念,通过Z-测试进行特征选择.将该方法应用于汉语的义项排歧中,实验表明，该算法具有较高的计算效率和正确率.
关键词　语言模型,最大熵模型,参数估计,特征选择,互信息,Z-测试.
中图法分类号　TP18
An Improved Maximum Entropy Language Model and Its Application
LI Juan-zi HUANG Chang-ning
(Department of Computer Science and Technology Tsinghua University Beijing 100084)
(State Key Laboratory of Intelligent Technology and Systems Tsinghua University Beijing 100084)
Abstract　 The maximum entropy approach is proved to be expressive and effective for the statistics language modeling, but it suffers from the computational expensiveness of the model building. An improved maximum entropy approach which makes use of mutual information of information theory to select features based on Z-test is proposed. The approach is applied to Chinese word sense disambiguation. The experiments show that it has higher efficiency and precision.
Key words　Language model, maximum entropy model, parameter estimation, feature selection, mutual information, Z-test.
　　语言模型试图反映、记录并使用自然语言中存在的规律［1］.近几年在自然语言处理的研究过程中发现,最大熵方法是一种建立统计语言模型的有效方法,具有较强的知识表达能力.但是,在用现有的最大熵方法建立统计模型时存在计算量过大的问题［2］.本文针对这一问题,提出了一种新的特征选择算法.算法使用互信息的概念,通过Z-测试的方法进行特征选择.在特征选择过程中,用新建模型与引用模型的Kullback-Leibler距离来调整所选出的特征.实验表明,这种算法具有较高的计算效率.
　　本文首先叙述最大熵原理,介绍已有的参数估计和特征选择方法,并对其进行评价;然后给出使用Z-测试的特征选择算法,最后将这种改进的最大熵方法应用于汉语的义类排歧中.
1 最大熵原理
　　最大熵原理最初是由E.T.Jayness在1950年提出的,Della Pietra等人于1992年首次将它应用于自然语言处理的语言模型建立中［1］.本文只是简单介绍最大熵原理,更详细的叙述请见参考文献［3,4］.
　　直觉上讲,最大熵原理的基本思想是:给定训练数据即训练样本,选择一个与所有的训练数据一致的模型.比如在英语中,对于一个具有词性歧义的词条,如果发现一个名词前为一个冠词的概率为50%,而在名词前为一个形容词的概率为30%,则最大熵模型应选择与这些观察一致的概率分布.而对于除此之外的情况,模型赋予的概率分布为均匀分布.
1.1 问题描述
　　设随机过程P所有的输出值构成有限集Y,对于每个输出y∈Y,其生成均受上下文信息x的影响.已知与y有关的所有上下文信息组成的集合为X,则模型的目标是:给定上下文x∈X,计算输出为y∈Y的条件概率,即对p(y｜x)进行估计.p(y｜x)表示在上下文为x时,模型输出为y的条件概率,其中y∈Y且x∈X.如:对于义类歧义问题,集合Y是具有义类歧义的某一词W的所有可能义类组成的集合,集合X为对词W的每次出现,为其选定的上下文环境所组成的集合.
1.2 训练数据
　　模型输入是经过人工排歧或从已标注过的语料库中抽取出的大量(x,y)训练样本,即对在语料库中有歧义的对象的每次出现,都已有确定的取值y及其对应的上下文环境x.可以用概率分布的极大似然对训练样本进行表示.即
(1)
其中freq(x,y)是(x,y)在样本中出现的次数.
1.3 特征、特征函数及约束
　　由问题描述可知,随机过程P与上下文信息x有关,但如果考虑所有与y同现的上下文信息,建立的模型会很繁琐,而且从语言知识上来讲,y的生成只与其上下文中的部分信息有关.因此,从x中找出对y的取值有用的知识才是模型所追求的目标.而这些有用的知识正是最大熵模型所要寻找的特征.
　　定义1. 特征
　　设x∈X且x=w1w2...wn,而c是x的一个子串（长度≥1）,若c对y∈Y具有表征作用,则称（c,y ）为模型的一个特征.
　　特征分为原子特征和复合特征.若串c的长度为1,则称（c,y）为原子特征,否则,称（c,y ）为复合特征.
　　定义2. 特征函数
　　特征函数是一个二值表征函数,表示（x′,y′)是否与特征（c,y）有关,定义（x′,y′)关于特征（c,y）的特征函数为
（2）
　　由以上定义可以看出,样本中出现在歧义对象周围的所有的词和该对象的确定值一起都可以作为模型的特征,因此,与模型有关的候选特征组成的集合会很大.但模型选出的特征只是真正对模型有用的特征,是候选特征集合的一个子集,它能较完整地表达训练语料中的数据.由此引入约束.
　　定义3. 约束
　　设(f)为特征f对于经验概率分布(x,y)的数学期望,表示为
,（3）
p(f)为特征f对于由模型确定的概率p(x,y)的数学期望,表示为
,（4）
而p(x,y)=p(x)p(y｜x),令p(x)=(x),则限定所求模型的概率为在样本中观察到事件的概率,而不是所有可能出现的事件的概率.若f对模型有用,则令
p(f)=(f),　　（5）
称式(5)为约束.
1.4 最大熵原理
　　假设存在n个特征fi（i=1,2,...,n）,则模型属于约束所产生的模型集合,即
C={p∈P｜p(fi)=(fi),i∈{1,2,...,n}},（6）
而满足约束条件的模型有很多,模型的目标是产生在约束集下具有最均匀分布的模型,而条件概率p(y｜x)均匀性的一种数学测量方法为条件熵,定义为
,（7）
其中0≤H(p)≤log｜y｜.
　　最大熵原理. 若在允许的概率分布C中选择模型,具有最大熵的模型px∈C即为所选模型.即
.（8）
2 参数估计及特征选择
　　利用最大熵建立语言模型的过程分为两步:特征选择和参数估计.特征选择的任务是选出对模型有表征意义的特征;参数估计用最大熵原理对每一个特征进行参数估值,使每一个参数与一个特征相对应,以此建立所求模型.
2.1 参数估计
　　Danroch和Ratcliff于1972年提出一个称为GIS（generalized iterative scaling algorithm）的算法,该算法是一般的迭代算法.Della Pietra等人于1995年根据所处理的问题对算法作了进一步改进,提出了IIS（improved iterative scaling algorithm）算法,算法设满足最大熵条件的概率p(x,y)具有Gibbs分布的形式
，（9）
其中
,（10）
Zλ(x)为归一常量,保证对所有x,.
2.2 特征选择
　　无论GIS还是IIS的参数估计方法提供的均是求解λ值的方法,保证以λ建立的模型不含有任何额外的假设.这两个算法并不能保证模型所含特征是具有良好表征意义的特征,因此,在建立模型中十分重要的一部分工作是特征选择.Della Pietra等人提出的原子特征算法思想是:开始设特征集S为空,此后不断向S中增加特征,每次增加的特征由训练数据决定.以训练数据的对数似然作为特征选择的依据,即若S为已选中的特征集,f～为候选特征,用L(ps)表示由S决定的模型的对数似然,则每次选出的f～应该为使公式
ΔL(s,)≡L(ps∪)-L(Ps),（11）
增加最多的特征.其中
.（12）
　　该算法的致命弱点是计算量大,每选一个特征都需要对所有的候选特征调用IIS算法,对λ重新计算,并且要对训练数据的对数似然进行计算,然后选出一个使模型的对数似然增加最多的特征,这几乎是不可操作的.为了使特征选择过程可行,Della Pietra等人又给出了一系列优化算法,如在向模型中加入一个新的特征时,保持前面IIS过程估计的λ值不变,只用IIS计算新加入特征的对应值,当所有特征选出后,重新调用一次IIS过程,对所有λ进行一次重新计算,这种方法虽然可以加快特征选择的过程,但不能保证每次加入模型的特征是最好的.
3 使用Z-测试的特征选择算法
　　建立最大熵模型的关键是要选出具有预期作用的特征,只有这样才能保证得到的解是对模型最有用的解.虽然Della Pietra等人的原子特征选择方法,可以选出最好的有预期作用的特征,但这种方法完全建立在数学运算的基础之上,存在着计算量大的问题.
既然特征选择的目的是要选出对模型具有预期作用的上下文信息,则这个信息与所要预期的值具有较密切的搭配关系.本文正是从这一假设出发,提出一种使用互信息概念,采用Z-测试的方法来进行特征选择的算法.
3.1 原子特征选择的问题描述
　　已知训练数据中的N个训练样本(x1,y1),(x2,y2),...,(xN,yN),其中xi∈X且yi∈Y,设A=｛aj｜aj是xi的子串,且aj的长度为1,i=1,2,...,N｝,则原子特征选择的意义是从A中选出能够充分表征Y的不同取值的最小特征集F=｛f1,f2,...,fn｝,其中fi=(ai,yi)（ai∈A,yi∈Y,i=1,2,...,n）为原子特征.
　　在此,我们采用Kullback-Leibler距离来测定特征所确定模型的质量.设是由训练语料确定的概率模型,p为由特征集确定的模型,则Kullback-Leibler距离定义为
 .(13)
最终要找的模型p为
.(14)
3.2 利用Z-测试进行原子特征选择的依据
　　(1) 互信息［5］可衡量搭配的强度
特征选择的目的是要选出对模型具有预期作用的上下文信息,所以这个信息应与所预期的值具有较密切的搭配关系,而信息论中的互信息正是测量搭配强度的一个物理量.对应于我们要解决的问题为:若某一上下文信息对y有表征意义,则y与该上下文的互信息较大.
　　(2) Z-测试［6］可作为互信息的一个测度
虽然互信息可以作为描述搭配强度的物理量,但是,如果特征选择直接确定选择互信息大于某一阈值的上下文信息为特征时,则对不同互信息的分布,设定的阈值也不相同,这样,算法难以操作.而Z-测试可以将互信息的分布进行标准变换,将其变为标准的正态分布,这样,不论互信息如何分布,都可以从一个统一的阈值开始进行求解.
3.3 原子特征选择算法
　　输入:训练样本（x1,y1）,（x2,y2）,...,（xN,yN）;
　　输出:特征矩阵Dm×n.其中m=｜Y｜,即y的所有可能取值的个数;n=｜｛aj｜aj是xi的子串,且aj的长度为1,i=1,2,...,N｝｜,即与y的不同值同现的候选特征集合中的元素个数.
 .
　　过程
　　步骤1.
　　.由样本（x1,y1）,（x2,y2）,...,（xN,yN）得到Y中元素与A中元素的同现次数矩阵Fm×n,其中
　　fij=f(yi,aj)=｜{(xk,yk)｜aj是xs的子串,且长度为1,yk=yi,s=1,2,...,N}｜.
　　.计算互信息矩阵Im×n,其中
 ,
其中M为语料库的大小,而f(yi)和f(aj)分别为yi和aj在语料库中出现的次数.由互信息的定义可知:当Iij0时,yi与aj完全并列,因此,（aj,yi）可作为模型的特征.
　　步骤2.
　　对每个yi,
　　.计算yi的互信息均值
;(15)
　　.计算其均方差
.(16)
　　步骤3.
　　用Z-测试对每个yi生成表征向量(di1,di2,...,dim);
　　.对每个yi（i=1,2,...,n ）,
　　.对每个aj,计算
 ,(17)
　　* 若zij＞T,则aj与yi为选出的一个特征,令dij=1;否则dij=0;
　　步骤4.
　　.用选出的原子特征集合S={f1,f2,...,fk}调用IIS算法,得到〈Z,λ1,λ2,...,λk〉;
　　.用公式(9)和(10)计算p(y｜x).
　　步骤5.
　　.计算由p(y｜x)确定的模型与经验概率分布模型(y｜x)的距离D(p‖);
　　.用D(p‖)与上次的D′(p‖)比较;若D-D′＜ε,则过程结束;否则,T=T-ΔT,转步骤3.
3.4 阈值T的确定
　　(1) T初值的确定
　　从算法可以看出,在经过式(17)的运算后,已将互信息的分布变为正态分布.从概率论可知:正态分布在区间［-3,+3］内,其整个概率覆盖度可达99%左右.因此,T可以在［-3,+3］内进行取值.因为开始时要选出表征意义大的特征,所以应赋予T一个较大的初值.
　　(2) T阈值的变化
　　初值确定后,以后每次以一个步长ΔT减少,这就意味着每次根据T选中的特征不是一个,而是具有同等表达程度的一个候选特征子集,且选出的子集中包含上一次选出的特征集合.因此,在进行下一次的参数估计时,对于以前的特征其初值可以从上次确定的值开始,这样做可以节省大量运算时间.特征选择过程最终得到的特征集合是它所确定的模型的D(p‖)较小且具有较一般表征意义的集合.
3.5 两个原子特征选择算法的计算量比较
　　(1) Della Pietra的特征选择算法的计算量分析
该特征选择算法每次确定一个特征时的计算量由两部分组成,即调用IIS对每一候选特征进行参数估计和计算模型的对数似然.总的计算量可表示为
C1=n*(IIS1+L1)+(n-1)*(IIS2+L2)+...+(n-k)*(IISk+Lk).（18）
其中n为候选特征集合中候选特征的个数,k为最终特征集合中的特征个数,IISi和Li分别表示在选第i个特征时参数估计的计算量和对数似然的计算量.
　　设IISmin为在k次特征选择过程中,参数估计过程所需的最少时间,则
C1≥n*(IISmin+L1)+(n-1)*(IISmin+L2)+...+(n-k)*(IISmin+Lmin).
由公式(12)可知,L1=L2=...=Lk,则
C1≥n*(IISmin+L1)+(n-1)*(IISmin+L1)+...+(n-k)*(IISmin+L1).
= .（19）
　　(2) 本文提出的特征选择算法的计算量分析
　　本文提出的特征选择算法每次入选的特征有多个,整个过程的计算量由3部分组成,即互信息的计算量、参数估计的计算量及Kullback-Leibler距离的计算量.总的计算量为
C2=O(m*n)+(IISa1+Da2)+(IISa2+Da2)+...+(IISai+Dai).（20）
其中O(m*n)为互信息的计算量,ai为第i次选中的特征个数,IISai和Dai分别表示在第i次特征选择时参数估计的计算量和Kullback-Leibler距离的计算量.
　　设IISmax为在i次特征选择中参数估计时间的最大量,则
C2≤O(m*n)+(IISmax+Da1)+(IISmax+Da2)+...+(IISmax+Dai).
由公式(13)可知,Da1=Da2=...=Dai,则
C2≤O(m*n)+i.IISmax+i.Da1.(21)
　　由算法可知,m《n,i＜k及k《n,IISmin与IISmax的计算复杂度属于同一数量级,而对数似然的计算复杂度与Kullback-Leibler距离的计算量大致相同,所以,本文提出的特征选择算法所需的运算量小于Della Pietra等人提出的特征选择算法所需的运算量.
4 改进最大熵模型的应用及实验结果
4.1 基于最大熵原理的义类排歧
　　作者将上面描述的建立最大熵模型的方法应用于解决汉语文本中的义类排歧问题.
　　模型输入: 已知多义词w的由N个样本组成的样本空间:(x1,y1),(x2,y2),...,(xn,yn),(xi,yi)表示当上下文信息为xi时,w的义类为yi.xi为yi的上下文环境.模型的目标是利用最大熵原理建立学习模型p(y｜x),其含义为在上下文为x时输出义类为y的概率.
　　模型输出: 特征集及对应参数集,即〈S,λ〉;其中S={f1,f2,...,fn}且λ={λ1,λ2,...,λn}.
4.2 实验过程
　　(1) 语料库和义类词典
在实验中,将2 000万字已经进行了词切分和词性标注的《人民日报》语料库作为系统的数据来源,以在语料库中常出现的词性为动词或名词的高频多义词作为义类排歧的对象.在对多义词进行语义标注时,采用的是《同义词词林》中的义类代码,代码由大、中、小三级组成,如“建”有两个义类“Hc05”和“Hd01”,它们分别代表两种不同的意义.
　　(2) 样本数据
根据最大熵模型,义类排歧模型的样本空间为从语料库中抽出包含某个多义词的词及其周围的上下文环境,然后对每个样本进行人工排歧,形成样本空间.在样本数据的准备过程中,我们做了两方面的工作.(1)确定义类排歧的对象为同一词性内的义类歧义词;(2)对多义词周围所取的上下文的长度的原则定为:以句子为单位,在一句中选该词周围前后各7个词.以“打”、“建”和“获”这3个具有义类歧义的词作为实验对象,它们在语料库中出现的次数及候选特征的个数见表1.
　　(3) 特征选择和参数估计
按本文提出的特征选择算法最终产生的特征个数和T的值见表2.
表1 　　　　　　　　　　　　表2

义类歧义词样本个数候选特征个数义类歧义词最后T的值特征个数
打1 6421 145打1.117377
建1 9283 029建1.216402
获2 6823 766获1.343285

该模型产生的有关“打”的部分特征见表3.
表3

y=“Fa10”且“井”∈x
y=“Fa10”且“深”∈x
y=“Fa10”且“扩孔”∈x
y=“Hi44”且“死”∈x
y=“Hi44”且“士兵”∈x
y=“Hi44”且“致残”∈x

　　(4) 用最大熵模型进行义类排歧的过程及结果
　　具体过程为:
　　. 找出含有指定多义词w的上下文（x,w）,其中x为多义词w的上下文环境;
　　. 根据模型学习到的关于w的参数集〈S,λ〉,用公式（9）和（10）计算w的各个义类在x下的条件概率p(yi｜x),其中yi∈Y.
　　. 取条件概率较大者对应的义类为所选义类.
在此,我们分别采用封闭测试和开放测试两种方法对模型进行测试,测试正确率定义为
 .
得到的测试结果见表4.从表中数据可以看出,用本文提出的特征选择算法建立的最大熵模型可以保证有较高的排歧准确率.
表4

　封闭测试开放测试
样本个数正确率(%)样本个数正确率(%)
打10089.55083.7
建10093.25090.6
获10091.85089.1

5 结束语
　　本文提出一种改进的最大熵方法,该方法利用互信息的概念,使用Z-测试方法进行特征选择,并以建立模型与经验模型的Kullback-Leibler距离作为过程的结束条件,因此,可以保证模型的准确性.将模型用于汉语的义类排歧中,取得了较高的排歧正确率.本文提出的方法还可用于词性标注、句子边界识别等问题.
致谢　本文的研究得到国家自然科学基金资助，此项目编号为69433010.
　　本文研究得到国家自然科学基金重点项目资助.作者李涓子,女,1964年生,博士生,主要研究领域为计算机语言学.黄昌宁,1937年生,教授，博士生导师,主要研究领域为计算语言学,人工智能.
　　本文通讯联系人:李涓子，北京100084,清华大学计算机科学与技术系
作者单位：清华大学计算机科学与技术系 北京　100084
　　　　　清华大学智能技术与系统国家重点实验室 北京 100084
　　　　　E-mail: ljz@s1000e.cs.tsinghua.edu.cn
参考文献
［1］Ronnald Rosenfeld. A maximum entropy to adaptive statistical language learning. Computer Speech and Language, 1996,10(3):187～228 
［2］Andrei Mikheev et al. Collocation Lattices and maximum entropy models. In: Zhou Joe ed. Proceedings of the 5th Workshop on Very Large Corpora. Beijing: Association for Computational Lingnistics, 1997. 216～230
［3］Berger A L, Della Pietra S et al. A maximum entropy approach to natural language processing. Computational Linguistics, 1996,22(1):40～72
［4］Della Pietra S, Della Pietra V et al. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligent, 1997,19(4),380～393
［5］Church K, Hanks P. Word association norms, mutual information, and lexicography. Computational Linguistics, 1990,16(1),22～29
［6］Frank Smadja. Retrieving collocation from text: Xtract. Computational Linguistics, 1993,19(1):143～175
（1998-03-12收稿）
