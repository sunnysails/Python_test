自动化学报
ACTA AUTOMATICA SINICA
1997年　第23卷　第4期　Vol.23　No.4　1997



小脑模型神经网络改进算法的研究1）
刘慧　许晓鸣　
　　摘　要　该文介绍了小脑模型神经网络的基本原理，在分析Albus［1］算法的基础上，指出了该算法在批量学习时的缺陷.针对批量学习提出了相应的改进算法，并证明了该算法的收敛性，仿真结果表明了该改进算法具有收敛速度快的特点.
　　关键词　神经网络， 杂凑编码， 联想网络， CMAC.
AN IMPROVED CMAC NEURAL NETWORK ALGORITHM
LIU HUI XU XIAOMING 
(Department of Automation , Shanghai Jiaotong University, Shanghai 200030)
Abstract　The basic principle of CMAC (Cerebelllar Model Articulation Controller) is introduced. Based on a detail analysis of Albus algorithm［1］, the paper points out its drawback in batch learning. An improved algorithm is proposed and theoretical proofs are also given. Simulation results show that the improved method has higher speed and better convergence than the original.
Key words　 Neural network, hash-coding, associative network, CMAC.
1　引言
　　自1982年Hopfield发表了关于反馈神经网络的文章以及Rumelhart等人发表了专著PDP以来，在世界范围内掀起了研究神经网络的热潮.近年来，人工神经元网络和人工智能在控制界中的应用研究正在兴起和蓬勃发展，特别是神经网络具有充分逼近任意复杂非线性函数的能力，为解决复杂的非线性问题开辟了一条控制的具有特殊联想功能的神经网络CMAC(Cerebellar Model Articulation Controller).
　　CMAC神经网络是由J.S.Albus［2］在1975年提出的.它与Perceptron网相似，虽然从每个神经元看其关系是一种线性关系，但从结果总体看CMAC模型适合于非线性的映射关系.同时它的算法是十分简单的δ算法，所以速度很快.它把输入在一个多维状态空间中的量，映射到一个比较小的有限区域.只要对多维状态空间中部分样本进行学习，就可达到轨迹学习和控制的解，因此特别适合于机器人的轨变学习控制，实时学习控制，非线性函数映射，以及模式识别等领域.CMAC具有自适应的作用，并且易于硬件化实现.
2　CMAC的基本原理
　　CMAC的简单结构模型如图1所示，输入空间S由所有可能的输入向量Si组成，CMAC网络将其接受到的任何输入，通过感知器M映射到一个很大的联想存储器A中的c个单元.输入空间邻近的两个输入向量在存储器A中有部分重叠的单元；距离越近，重叠越多.反之，若输入空间远离的两个输入向量在A中并不重叠.对一个实际系统，输入空间中的向量数是很大的，例如，某系统有10个输入，而每个输入可取100个值，则在输入空间有10010个向量，A的存储量需要10010个单元.由于绝大多数学习问题并不包括所有输入空间中的状态，故所需的存储空间可通过常用的计算机存储压缩技术――随机杂凑编码(Hash-coding)技术映射到一个小得多的物理可实现的存储器A′.任何CMAC网络的输入激活c个真实存储位置，而这些位置的值被相加得到输出向量P.


图1　CMAC网格的模型结构
3　Albus的CMAC算法
　　设共有P个训练样本，联想向量ai为M→A的映射，CMAC网络对于第i个训练样本的输出为f(Si）=wTai，理想输出为di，误差信号δi=di-f(si)，CMAC是按δ学习律调整网络的权值，是在梯度法的基础上采用LMS算法得到的［2］

(1)
其中β为学习步长.
　　当我们在第l次循环时对第k个样本进行训练时，产生的修整量为β/c，其中为第k个样本的输出误差.因CMAC网络内在的泛化能力，在输入空间相近的向量在实际存储器A′中有重叠，故该修正量势必影响其它样本的输出，如它使第i个样本的新输出为

(2)
其中cik为第i,k个样本在A′中的重叠单元数，显然cik=cki.经I次循环后，输入样本k产生的修正量对其本身的累积输出误差为，与之相关的每个权的修正量为Δk＝Ek／c，Δk称为累积权误差.累积权误差对第i个样本的输出贡献为cikΔk.当CMAC算法收敛，→0，则Δk→const.若我们由Δi得wi，则在网络算法中可换一个角度将累积权误差Δi而不是权wi视为需要学习的变量，这样CMAC的收敛也可转化为Δi的收敛问题.设网络初始权为零，则对每个输入向量si，网络的输出为

(3)
学习的目的是使f(si)=di，即，写成矩阵形式
CΔ＝D
(4)
令　A=［a1 a2…ap］T，则
AW=D
(5)
　　通常神经网络权的学习有逐一和批量两种方法，设训练样本集为｛（s1,d1),(s2,d2),…，(sp,dp)｝.逐一学习是每输入一个样本便对整个网络的权进行修正.即先对(s1,d1)进行学习，利用输出误差来对网络的权进行修正，使网络的对应关系满足f(s1)=d1，设此时网络的权为w1.然后以w1为基础，对样本(s2,d2)进行学习，直到权值w2满足f(s2)=d2.但是一般情况下，w1≠w2，故此时未必有f(s1)=d1成立.所以逐一学习的方法会出现学了新的，忘了旧的“遗忘”现象.为了克服这个缺点需采用反复循环学习，但这样又会带来收敛慢的问题.我们建议用批量学习的方法来对权进行更新，将样本一一输入，为一批样本输入后用总的误差来修整权，这种方法可以克服“遗忘”的缺点，又有较快的收敛速度.
　　Albus本人并未给出CMAC算法收敛性的证明，Wong［3］和Parks［4］从线性方程组迭代解的角度，Wong［5］还从频域的角度阐述了Albus算法的收敛性问题.他们的研究都是针对逐一学习的，对批量学习的情况并未进行研究.作者针对批量学习情况进行了一些探索，指出了Albus算法在逐一学习时适用，而在批量学习时易发散的缺陷，并提出了相应的改进算法.
4　批量学习时Albus算法的缺陷及改进算法
　　定理1.对于多输入输出目标中的一组训练样本，如果输入空间被量化后不存在两个不同训练样本激活相同一组神经元的情况，则Albus算法采用批量学习方法时的收敛条件为
　　证明.当Albus算法采用批量学习时，累积权误差的修正公式为

(6)
其中

(7)
　　为方便起见，设输入输出空间均为一维，所有训练样本的量化值都在［0，R］上，则所需神经元为R+c-1个.
　　当收敛到Δ*时，由式(6)得

(8)
令　，得

(9)
　　e为一有限长的N1点非周期序列，N1＝R+2c-2.将其作周期延拓，e可写成Fourier级数形式

(10)
　　将(10)代入(9)并考虑(7)可得

(11)
其中

(12)
c一般取为32～256，故sin(cω)/c可忽略.若ω≠2π，则

(13)
　　当且仅当｜H(ejω)｜＜1时算法收敛，在一个数字频率周期(2π)内｜H(ejω)｜＜1，即

(14)
　　当ω=2π时，ω∈Ker(A)，不影响w和Δ的收敛.所以Albus算法采用批量学习时的收敛条件为
　　推论1.对于多输入输出目标中的一组训练样本，设所有训练样本的量化值都在［0，R］上，感受野宽度为c.如果输入空间被量化后不存在两个不同训练样本激活相同一组神经元的情况，则Albus算法采用批量学习方法时的收敛条件为

　　证明.　将代入式(14)，得
　　　

(15)
　　由Fourier级数(10)知，k的取值为［0,N,-1］间的连续整数，而采用Albus算法批量学习时的收敛条件为，k在和两段取值时算法不收敛.N1越大，即量化取值数R和感受野宽度c越大，不收敛的点越多.
　　为了克服Albus算法对批量学习不太适合的缺陷，提出了一种改进的CMAC算法把式(1)修改为

(16)
其中q为批量学习一次每个权平均被更新的次数.
　　定理2.对于多输入输出目标中的一组训练样本，如果输入空间被量化后不存在两个不同训练样本激活相同一组神经元的情况，改进CMAC算法采用批量学习方法时的收敛条件为2ctg-1q＜ω＜2（π-ctg-1q).
　　证明.　改进CMAC算法采用批量方法对权进行更新时，相应的算法收敛证明中的公式(13)变为

(17)
　　当且仅当｜H(ejω)｜＜1时算法收敛，在一个数字频率周期(2π）内｜H(ejω)｜＜1，即
2ctg－1q＜ω＜2(π-ctg-1q).
(18)
　　当ω=2π时，ω∈Ker(A)，不影响w和Δ的收敛.所以改进CMAC算法采用批量学习时的收敛条件为2ctg-1q＜ω＜2(π-ctg-1q).
　　讨论.由于CMAC网络所固有的局域泛化能力，每个输入向量使其对应量化值周围c个感知器同时被激励；相应地，当一批样本被学习一遍后，每个网络的权也不止一次被更新.所以minq=2.当q取最小值2时，由(18)得改进算法的收敛条件为0.93＜ω＜5.35弧度，改进算法的收敛范围比Albus算法(14)宽，q越大改进算法的收敛范围将越宽.
　　推论2.对于多输入输出目标中的一组训练样本，设所有训练样本的量化值都在［0，R］上，感受野宽度为c.如果输入空间被量化后不存在两个不同训练样本激活相同一组神经元的情况，改进CMAC算法采用批量学习方法时的收敛条件为
　　证明.　将代入式(18)即可得改进算法采用批量学习时的收敛条件为

(19)
　　讨论.将(19)进一步展开为

(20)
其中

(21)
　　当q取最小值2时，ctg-1q=0.46弧度，而反余切函数在区间［0，π］内是递减的，所以ctg-1q＜0.46，从而b1＞0,b2＞0.可见改进算法的收敛范围比Albus算法宽.q越大则b1和b2越大，改进算法的收敛范围也将越宽.
　　改进CMAC算法对逐一学习也是适用的，其收敛性证明可参见文献［4］，其中累积权误差的修正类似Jocobi迭代法解线性方程组.
5　仿真结果
　　设某CMAC网络用来实现下列一维非线性函数的映射，

(22)
其中输入变量si∈［0，180］，分辨率为1，所有训练样本的量化值都在［0，R］上，R=180.CMAC网络的感受野宽度c=32,批量学习一次每个权平均被更新的次数q取为6.则N1=242，ctg-1q=0.17.按Δs=5的间隔均匀取点，由式(22)进行理论计算，可得37组输入输出的样本数据，运用这些样本数据分别按Albus算法和本文的改进算法对CMAC神经网络进行训练.为了显示网络的泛化能力，计算网络输出时采用了不同于训练时的数据Δs=6.由式(15)计算得Albus算法批量训练时的收敛条件为60＜k＜182;而由式(19)知12＜k＜230时本文的改进算法批量训练时收敛，易见改进后的算法比Albus算法收敛条件宽.
　　图2所示为逐一学习时的误差曲线，两种算法都可取得较好的收敛效果，而本文提出的改进方法收敛速度更快一些.图3是在批量学习时的误差曲线，由该图可见，Albus算法易导致发散，而采用本文的改进算法可以获得收敛较快的效果.


图2　逐一学习误差曲线


图3　批量学习误差曲线
6　结论
　　本文在对Albus算法研究的基础上，提出了适合于批量学习的改进算法，并在理论上证明了该算法的收敛性，仿真结果表明了该算法在逐一和批量学习时都有较好的收敛性和快速性.
1)　留学回国人员科研基金资助课题.
作者简介:刘　慧　1968年生，1996年获上海交通大学自动控制理论及应用专业博士学位.现为上海交通大学自动化系讲师.主要研究兴趣是智能控制，学习控制及神经网络在控制中的应用.
　　许晓鸣　1957年生，现任上海交通大学自动化系教授、博士生导师.主要研究兴趣是智能控制和复杂工业系统预测控制等方面的研究.
作者单位:上海交通大学自动化系　上海　200030
参考文献
［1］　Albus J S. Data storage in the cerebellar model articulation controller (CM.AC). Trans. ASME, J.Dyn. Syst. Meas. Contr., 1975,97:228―233.
［2］　Albus J S. A new approach to manipulator control: The cerebellar model articulation Controller (CMAC). Trans. ASME, J.Dyn. Syst. Meas. Contr., 1975, 97:220―227.
［3］　Wong Y F,Sideris A. Learning convergence in the cerebellar model articulation controller .IEEE Trans. on Neural Networks, 1992, 3(1):115―121.
［4］　Parks P C, Militzer J. Convergence properties of associative memory storage for learning control system. Automation and Remote Control, 1989, 50(2):254―286.
［5］　Wong Y F. CMAC learning is governed by a single parameter. IEEE Int. Conf. on NN, 1993, 1439―1443.
收稿日期　1995-04-10
